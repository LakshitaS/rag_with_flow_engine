<!DOCTYPE html><html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="DC.Publisher" content="IBM Corporation">
  <meta name="DC.rights" content="Â© Copyright IBM Corporation {{ copyright.years }}">
  <meta name="IBM.Country" content="ZZ">
  <meta name="DC.Date" content="{{ lastupdated }}">
  <meta name="keywords" content="{{ keywords }}">
  <meta name="subcollection" content="{{ subcollection }}">
  <meta name="description" content="You can train your own models for text classification using strong classification algorithms from three different families:">
  <meta name="content.type" content="topic">
  <meta name="tags" content="{{ services }}">
  <meta name="account.plan" content="{{ account-plan }}">
  <meta name="completion.time" content="{{ completion-time }}">
  <meta name="version" content="{{ version }}">
  <meta name="deployment.url" content="{{ deployment-url }}">
  <meta name="industry" content="{{ industry }}">
  <meta name="compliance" content="{{ compliance }}">
  <meta name="use.case" content="{{ use-case }}">
  <meta name="source.format" content="markdown">




  <!-- Licensed Materials - Property of IBM -->
  <!-- US Government Users Restricted Rights -->
  <!-- Use, duplication or disclosure restricted by -->
  <!-- GSA ADP Schedule Contract with IBM Corp. -->

  <title>Classifying text with a custom classification model</title>
<link rel="canonical" href="https://www.ibm.com/docs/en/watsonx-as-a-service?topic=models-classifying-text-custom-classification-model"><meta name="viewport" content="width=device-width,initial-scale=1"></head>

<body>
  <main role="main">
    <article aria-labelledby="classifying-text-with-a-custom-classification-model" role="article">
      <style>
        .midd::after {
          content: "\A\00A0\00A0";
          white-space: pre
        }
      </style>
      <section id="section-classifying-text-with-a-custom-classification-model">
        <h1 id="classifying-text-with-a-custom-classification-model">Classifying text with a custom classification model</h1>
        <p>You can train your own models for text classification using strong classification algorithms from three different families:</p>
        <ul class="shortdesc">
          <li>Classic machine learning using SVM (Support Vector Machines)</li>
          <li>Deep learning using CNN (Convolutional Neural Networks)</li>
          <li>A transformer-based algorithm using a pre-trained transformer model:
            <ul>
              <li>Runtime 23.1: Slate IBM Foundation model</li>
              <li>Runtime 22.x: Google BERT Multilingual model</li>
            </ul>
          </li>
        </ul>
        <p>The Watson Natural Language Processing library also offers an easy to use Ensemble classifier that combines different classification algorithms and majority voting.</p>
        <p>The algorithms support multi-label and multi-class tasks and special cases, like if the document belongs to one class only (single-label task), or binary classification tasks.</p>
        <div class="note note"><span class="notetitle">Note:</span>
          <md-block>
            <p>Training classification models is CPU and memory intensive. Depending on the size of your training data, the environment might not be large enough to complete the training. If you run into issues with the notebook kernel during training, create
              a custom notebook environment with a larger amount of CPU and memory, and use that to run your notebook. Especially for transformer-based algorithms, you should use a GPU-based environment, if it is available to you. See <a href="create-customize-env-definition.html">Creating your own environment template</a>.</p>
            <p></p>
          </md-block>
        </div>
        <p></p>
        <p>Topic sections:</p>
        <ul>
          <li><a href="#input-data">Input data format for training</a></li>
          <li><a href="#input-data-reqs">Input data requirements</a></li>
          <li><a href="#stopwords">Stopwords</a></li>
          <li><a href="#train-svm">Training SVM algorithms</a></li>
          <li><a href="#train-cnn">Training the CNN algorithm</a></li>
          <li><a href="#train-slate">Training the transformer algorithm by using the Slate IBM Foundation model</a></li>
          <li><a href="#train-huface">Training a custom transformer model by using a model provided by Hugging Face</a></li>
          <li><a href="#train-bert">Training the multilingual BERT algorithm</a></li>
          <li><a href="#train-ensemble">Training an ensemble model</a></li>
          <li><a href="#best-practices">Training best practices</a></li>
          <li><a href="#apply-model">Applying the model on new data</a></li>
          <li><a href="#choose-algorithm">Choosing the right algorithm for your use case</a></li>
        </ul>
        <section id="section-input-data">
          <h2 id="input-data">Input data format for training</h2>
          <p>Classification blocks accept training data in CSV and JSON formats.</p>
          <ul>
            <li>
              <p>The CSV Format</p>
              <p>The CSV file should contain no header. Each row in the CSV file represents an example record. Each record has one or more columns, where the first column represents the text and the subsequent columns represent the labels associated with
                that text.</p>
              <p>Note:</p>
              <ul>
                <li>The SVM and CNN algorithms do not support training data where an instance has no labels. So, if you are using the SVM algorithm, or the CNN algorithm, or an Ensemble including one of these algorithms, each CSV row must have at least one
                  label, i.e., 2 columns.</li>
                <li>The BERT-based and Slate-based Transformer algorithms support training data where each instance has 0, 1 or more than one label.</li>
              </ul>
              <pre class="codeblock"><code class="lang-txt hljs">Example 1,label 1
Example 2,label 1,label 2
</code></pre>
            </li>
            <li>
              <p>The JSON Format</p>
              <p>The training data is represented as an array with multiple JSON objects. Each JSON object represents one training instance, and must have a text and a labels field. The text represents the training example, and labels stores the labels associated
                with the example (0, 1, or more than one label).</p>
              <pre class="codeblock"><code class="lang-json hljs"><span class="hljs-punctuation">[</span>
    <span class="hljs-punctuation">{</span>
    <span class="hljs-attr">"text"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Example 1"</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"labels"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">"label 1"</span><span class="hljs-punctuation">]</span>
    <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span>
    <span class="hljs-punctuation">{</span>
    <span class="hljs-attr">"text"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Example 2"</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"labels"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">"label 1"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"label 2"</span><span class="hljs-punctuation">]</span>
    <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span>
    <span class="hljs-punctuation">{</span>
    <span class="hljs-attr">"text"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Example 3"</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"labels"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">]</span>
    <span class="hljs-punctuation">}</span>
<span class="hljs-punctuation">]</span>
</code></pre>
              <p>Note:</p>
              <ul>
                <li><code>"labels": []</code> denotes an example with no labels. The SVM and CNN algorithms do not support training data where an instance has no labels. So, if you are using the SVM algorithm, or the CNN algorithm, or an Ensemble
                  including one of these algorithms, each JSON object must have at least one label.</li>
                <li>The BERT-based and Slate-based Transformer algorithms support training data where each instance has 0, 1 or more than one label.</li>
              </ul>
            </li>
          </ul>
        </section>
        <section id="section-input-data-reqs">
          <h2 id="input-data-reqs">Input data requirements</h2>
          <p>For SVM and CNN algorithms:</p>
          <ul>
            <li>Minimum number of unique labels required: 2</li>
            <li>Minimum number of text examples required per label: 5</li>
          </ul>
          <p>For the BERT-based and Slate-based Transformer algorithms:</p>
          <ul>
            <li>Minimum number of unique labels required: 1</li>
            <li>Minimum number of text examples required per label: 5</li>
          </ul>
          <p>Note that the training data in CSV or JSON format is converted to a DataStream before training. Instead of training data files, you can also pass data streams directly to the training functions of classification blocks.</p>
        </section>
        <section id="section-stopwords">
          <h2 id="stopwords">Stopwords</h2>
          <p>You can provide your own stopwords that will be removed during preprocessing. Stopwords file inputs are expected in a standard format: a single text file with one phrase per line. Stopwords can be provided as a list or as a file in a standard
            format.</p>
          <p>Stopwords can be used only with the Ensemble classifier.</p>
        </section>
        <section id="section-train-svm">
          <h2 id="train-svm">Training SVM algorithms</h2>
          <p>SVM is a support vector machine classifier that can be trained using predictions on any kind of input provided by the embedding or vectorization blocks as feature vectors, for example, by <code>USE</code> (Universal Sentence Encoder) embeddings
            and <code>TF-IDF</code> vectorizers. It supports multi-class and multi-label text classification and produces confidence scores via Platt Scaling.</p>
          <p>For all options that are available for configuring SVM training, enter:</p>
          <pre class="codeblock"><code class="lang-python hljs"><span class="hljs-built_in">help</span>(watson_nlp.blocks.classification.svm.SVM.train)
</code></pre>
          <p>To train SVM algorithms:</p>
          <ol>
            <li>
              <p>Begin with these preprocessing steps:</p>
              <pre class="codeblock"><code class="lang-python hljs"><span class="hljs-keyword">import</span> watson_nlp
<span class="hljs-keyword">from</span> watson_core.data_model.streams.resolver <span class="hljs-keyword">import</span> DataStreamResolver
<span class="hljs-keyword">from</span> watson_nlp.blocks.classification.svm <span class="hljs-keyword">import</span> SVM

training_data_file = <span class="hljs-string">"&lt;ADD TRAINING DATA FILE PATH&gt;"</span>

<span class="hljs-comment"># Create datastream from training data</span>
data_stream_resolver = DataStreamResolver(target_stream_type=<span class="hljs-built_in">list</span>, expected_keys={<span class="hljs-string">'text'</span>: <span class="hljs-built_in">str</span>, <span class="hljs-string">'labels'</span>: <span class="hljs-built_in">list</span>})
training_data = data_stream_resolver.as_data_stream(training_data_file)

<span class="hljs-comment"># Load a Syntax model</span>
syntax_model = watson_nlp.load(<span class="hljs-string">'syntax_izumo_en_stock'</span>)

<span class="hljs-comment"># Create Syntax stream</span>
text_stream, labels_stream = training_data[<span class="hljs-number">0</span>], training_data[<span class="hljs-number">1</span>]
syntax_stream = syntax_model.stream(text_stream)
</code></pre>
            </li>
            <li>
              <p>Train the classification model using USE embeddings. See <a href="#use-embeddings">Pretrained USE embeddings available out-of-the-box</a> for a list of the pretrained blocks that are available.</p>
              <pre class="codeblock"><code class="lang-python hljs"><span class="hljs-comment"># download embedding</span>
use_embedding_model = watson_nlp.load(<span class="hljs-string">'embedding_use_en_stock'</span>)

use_train_stream = use_embedding_model.stream(syntax_stream, doc_embed_style=<span class="hljs-string">'raw_text'</span>)
<span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> doc_embed_style can be changed to `avg_sent` as well. For more information check the documentation for Embeddings</span>
<span class="hljs-comment"># Or the USE run function API docs</span>
use_svm_train_stream = watson_nlp.data_model.DataStream.<span class="hljs-built_in">zip</span>(use_train_stream, labels_stream)

<span class="hljs-comment"># Train SVM using Universal Sentence Encoder (USE) training stream</span>
classification_model = SVM.train(use_svm_train_stream)
</code></pre>
            </li>
          </ol>
          <section id="section-use-embeddings">
            <h3 id="use-embeddings">Pretrained USE embeddings available out-of-the-box</h3>
            <p>USE embeddings are wrappers around Google Universal Sentence Encoder embeddings available in TFHub. These embeddings are used in the document classification SVM algorithm.</p>
            <p>The following table lists the pretrained blocks for USE embeddings that are available and the languages that are supported. For a list of the language codes and the corresponding language, see <a href="watson-nlp-block-catalog.html#lang-codes">Language codes</a>.</p>
            <table>
              <caption>List of pretrained USE embeddings with their supported languages</caption>
              <thead>
                <tr>
                  <th>Block name</th>
                  <th>Model name</th>
                  <th>Supported languages</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><code>use</code></td>
                  <td><code>embedding_use_en_stock</code></td>
                  <td>English only</td>
                </tr>
                <tr>
                  <td><code>use</code></td>
                  <td><code>embedding_use_multi_small</code></td>
                  <td>ar, de, en, es, fr, it, ja, ko, nl, pl, pt, ru, tr, zh-cn, zh-tw</td>
                </tr>
                <tr>
                  <td><code>use</code></td>
                  <td><code>embedding_use_multi_large</code></td>
                  <td>ar, de, en, es, fr, it, ja, ko, nl, pl, pt, ru, tr, zh-cn, zh-tw</td>
                </tr>
              </tbody>
            </table>
            <p>When using USE embeddings, consider the following:</p>
            <ul>
              <li>
                <p>Choose <code>embedding_use_en_stock</code> if your task involves English text.</p>
              </li>
              <li>
                <p>Choose one of the multilingual USE embeddings if your task involves text in a non-English language, or you want to train multilingual models.</p>
              </li>
              <li>
                <p>The USE embeddings exhibit different trade-offs between quality of the trained model and throughput at inference time, as described below. Try different embeddings to decide the trade-off between quality of result and inference throughput
                  that is appropriate for your use case.</p>
                <ul>
                  <li><code>embedding_use_multi_small</code> has reasonable quality, but it is fast at inference time</li>
                  <li><code>embedding_use_en_stock</code> is a English-only version of <code>embedding_embedding_use_multi_small</code>, hence it is smaller and exhibits higher inference throughput</li>
                  <li><code>embedding_use_multi_large</code> is based on Transformer architecture, and therefore it provides higher quality of result, with lower throughput at inference time</li>
                </ul>
              </li>
            </ul>
          </section>
        </section>
        <section id="section-train-cnn">
          <h2 id="train-cnn">Training the CNN algorithm</h2>
          <p>CNN is a simple convolutional network architecture, built for multi-class and multi-label text classification on short texts. It utilizes GloVe embeddings. GloVe embeddings encode word-level semantics into a vector space. The GloVe embeddings
            for each language are trained on the Wikipedia corpus in that language. For information on using GloVe embeddings, see the open source GloVe embeddings documentation.</p>
          <p>For all the options that are available for configuring CNN training, enter:</p>
          <pre class="codeblock"><code class="lang-python hljs"><span class="hljs-built_in">help</span>(watson_nlp.blocks.classification.cnn.CNN.train)
</code></pre>
          <br>
          <p>To train CNN algorithms:</p>
          <pre class="codeblock"><code class="lang-python hljs"><span class="hljs-keyword">import</span> watson_nlp
<span class="hljs-keyword">from</span> watson_core.data_model.streams.resolver <span class="hljs-keyword">import</span> DataStreamResolver
<span class="hljs-keyword">from</span> watson_nlp.blocks.classification.cnn <span class="hljs-keyword">import</span> CNN

training_data_file = <span class="hljs-string">"&lt;ADD TRAINING DATA FILE PATH&gt;"</span>

<span class="hljs-comment"># Create datastream from training data</span>
data_stream_resolver = DataStreamResolver(target_stream_type=<span class="hljs-built_in">list</span>, expected_keys={<span class="hljs-string">'text'</span>: <span class="hljs-built_in">str</span>, <span class="hljs-string">'labels'</span>: <span class="hljs-built_in">list</span>})
training_data = data_stream_resolver.as_data_stream(training_data_file)

<span class="hljs-comment"># Load a Syntax model</span>
syntax_model = watson_nlp.load(<span class="hljs-string">'syntax_izumo_en_stock'</span>)

<span class="hljs-comment"># Create Syntax stream</span>
text_stream, labels_stream = training_data[<span class="hljs-number">0</span>], training_data[<span class="hljs-number">1</span>]
syntax_stream = syntax_model.stream(text_stream)

<span class="hljs-comment"># Download GloVe embeddings</span>
glove_embedding_model = watson_nlp.load(<span class="hljs-string">'embedding_glove_en_stock'</span>)

<span class="hljs-comment"># Train CNN</span>
classification_model = CNN.train(watson_nlp.data_model.DataStream.<span class="hljs-built_in">zip</span>(syntax_stream, labels_stream), embedding=glove_embedding_model.embedding)
</code></pre>
        </section>
        <section id="section-train-slate">
          <h2 id="train-slate">Training the transformer algorithm by using the IBM Slate model</h2>
          <p>The transformer algorithm using the pretrained Slate IBM Foundation model can be used for multi-class and multi-label text classification on short texts.</p>
          <p>The pretrained Slate IBM Foundation model is only available in Runtime 23.1.</p>
          <p>For all the options available for configuring Transformer training, enter:</p>
          <pre class="codeblock"><code class="lang-python hljs"><span class="hljs-built_in">help</span>(watson_nlp.blocks.classification.transformer.Transformer.train)
</code></pre>
          <p>To train Transformer algorithms:</p>
          <pre class="codeblock"><code class="lang-python hljs"><span class="hljs-keyword">import</span> watson_nlp
<span class="hljs-keyword">from</span> watson_nlp.blocks.classification.transformer <span class="hljs-keyword">import</span> Transformer
<span class="hljs-keyword">from</span> watson_core.data_model.streams.resolver <span class="hljs-keyword">import</span> DataStreamResolver
training_data_file = <span class="hljs-string">"train_data.json"</span>

<span class="hljs-comment"># create datastream from training data</span>
data_stream_resolver = DataStreamResolver(target_stream_type=<span class="hljs-built_in">list</span>, expected_keys={<span class="hljs-string">'text'</span>: <span class="hljs-built_in">str</span>, <span class="hljs-string">'labels'</span>: <span class="hljs-built_in">list</span>})
train_stream = data_stream_resolver.as_data_stream(training_data_file)

<span class="hljs-comment"># Load pre-trained Slate model</span>
pretrained_model_resource = watson_nlp.load(<span class="hljs-string">'pretrained-model_slate.153m.distilled_many_transformer_multilingual_uncased '</span>)

<span class="hljs-comment"># Train model</span>
classification_model = Transformer.train(train_stream, pretrained_model_resource)
</code></pre>
        </section>
        <section id="section-train-huface">
          <h2 id="train-huface">Training a custom transformer model by using a model provided by Hugging Face</h2>
          <div class="note note"><span class="notetitle">Note:</span> This training method is only available in Runtime 23.1.</div>
          <p>You can train your custom transformer-based model by using a pretrained model from Hugging Face.</p>
          <p>To use a Hugging Face model, specify the model name as the <code>pretrained_model_resource</code> parameter in the <code>train</code> method of <code>watson_nlp.blocks.classification.transformer.Transformer</code>. Go to <a href="https://huggingface.co/models">https://huggingface.co/models</a>            to copy the model name.</p>
          <p>To get a list of all the options available for configuring a transformer training, type this code:</p>
          <pre class="codeblock"><code class="lang-python hljs"><span class="hljs-built_in">help</span>(watson_nlp.blocks.classification.transformer.Transformer.train)
</code></pre>
          <p>For information on how to train transformer algorithms, refer to this code example:</p>
          <pre class="codeblock"><code class="lang-python hljs"><span class="hljs-keyword">import</span> watson_nlp
<span class="hljs-keyword">from</span> watson_nlp.blocks.classification.transformer <span class="hljs-keyword">import</span> Transformer
<span class="hljs-keyword">from</span> watson_core.data_model.streams.resolver <span class="hljs-keyword">import</span> DataStreamResolver
training_data_file = <span class="hljs-string">"train_data.json"</span>

<span class="hljs-comment"># create datastream from training data</span>
data_stream_resolver = DataStreamResolver(target_stream_type=<span class="hljs-built_in">list</span>, expected_keys={<span class="hljs-string">'text'</span>: <span class="hljs-built_in">str</span>, <span class="hljs-string">'labels'</span>: <span class="hljs-built_in">list</span>})
train_stream = data_stream_resolver.as_data_stream(training_data_file)

<span class="hljs-comment"># Specify the name of the Hugging Face model</span>
huggingface_model_name = <span class="hljs-string">'xml-roberta-base'</span>

<span class="hljs-comment"># Train model</span>
classification_model = Transformer.train(train_stream, pretrained_model_resource=huggingface_model_name)
</code></pre>
        </section>
        <section id="section-train-ensemble">
          <h2 id="train-ensemble">Training an ensemble model</h2>
          <p>The Ensemble model is a weighted ensemble of these three algorithms: CNN, SVM with TF-IDF and SVM with USE. It computes the weighted mean of a set of classification predictions using confidence scores. The ensemble model is very easy to use.</p>
          <section id="section-using-the-runtime-231-environment">
            <h3 id="using-the-runtime-231-environment">Using the <code>Runtime 23.1</code> environment</h3>
            <p>The GenericEnsemble classifier allows more flexibility for the user to choose from the three base classifiers TFIDF-SVM, USE-SVM and CNN. For texts ranging from 50 to 1000 characters, using the combination of TFIDF-SVM and USE-SVM classifiers
              often yields a good balance of quality and performance. On some medium or long documents (500-1000+ characters), adding the CNN to the Ensemble could help increase quality, but it usually comes with a significant runtime performance impact
              (lower throughput and increased model loading time).</p>
            <p>For all of the options available for configuring Ensemble training, enter:</p>
            <pre class="codeblock"><code class="lang-python hljs"><span class="hljs-built_in">help</span>(watson_nlp.workflows.classification.GenericEnsemble)
</code></pre>
            <p>To train Ensemble algorithms:</p>
            <pre class="codeblock"><code class="lang-python hljs"><span class="hljs-keyword">import</span> watson_nlp
<span class="hljs-keyword">from</span> watson_nlp.workflows.classification <span class="hljs-keyword">import</span> GenericEnsemble
<span class="hljs-keyword">from</span> watson_nlp.workflows.classification.base_classifier <span class="hljs-keyword">import</span> GloveCNN
<span class="hljs-keyword">from</span> watson_nlp.workflows.classification.base_classifier <span class="hljs-keyword">import</span> TFidfSvm
<span class="hljs-keyword">from</span> watson_nlp.workflows.classification.base_classifier <span class="hljs-keyword">import</span> UseSvm

training_data_file = <span class="hljs-string">"&lt;ADD TRAINING DATA FILE PATH&gt;"</span>

<span class="hljs-comment"># Create datastream from training data</span>
data_stream_resolver = DataStreamResolver(target_stream_type=<span class="hljs-built_in">list</span>, expected_keys={<span class="hljs-string">'text'</span>: <span class="hljs-built_in">str</span>, <span class="hljs-string">'labels'</span>: <span class="hljs-built_in">list</span>})
training_data = data_stream_resolver.as_data_stream(training_data_file)

<span class="hljs-comment"># Syntax Model</span>
syntax_model = watson_nlp.load(<span class="hljs-string">'syntax_izumo_en_stock'</span>)
<span class="hljs-comment"># USE Embedding Model</span>
use_model = watson_nlp.load(<span class="hljs-string">'embedding_use_en_stock'</span>)
<span class="hljs-comment"># GloVE Embedding model</span>
glove_model = watson_nlp.load(<span class="hljs-string">'embedding_glove_en_stock'</span>)

ensemble_model = GenericEnsemble.train(training_data, syntax_model,
                                           base_classifiers_params=[
                                       TFidfSvm.TrainParams(syntax_model=syntax_model),
                                               GloveCNN.TrainParams(syntax_model=syntax_model, glove_embedding_model=glove_model, cnn_epochs=<span class="hljs-number">5</span>),
                                               UseSvm.TrainParams(syntax_model=syntax_model, use_embedding_model=use_model, doc_embed_style=<span class="hljs-string">'raw_text'</span>)],
                                           use_ewl=<span class="hljs-literal">True</span>)
</code></pre>
          </section>
          <section id="section-pretrained-stopwords">
            <h3 id="pretrained-stopwords">Pretrained stopword models available out-of-the-box</h3>
            <p>The text model for identifying stopwords is used in training the document classification ensemble model.</p>
            <p>The following table lists the pretrained stopword models and the language codes that are supported (<code>xx</code> stands for the language code). For a list of the language codes and the corresponding language, see <a href="watson-nlp-block-catalog.html#lang-codes">Language codes</a>.</p>
            <table>
              <caption>List of pretrained stopword models with their supported languages</caption>
              <thead>
                <tr>
                  <th>Resource class</th>
                  <th>Model name</th>
                  <th>Supported languages</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><code>text</code></td>
                  <td><code>text_stopwords_classification_ensemble_xx_stock</code></td>
                  <td>ar, de, es, en, fr, it, ja, ko</td>
                </tr>
              </tbody>
            </table>
          </section>
        </section>
        <section id="section-best-practices">
          <h2 id="best-practices">Training best practices</h2>
          <p>There are certain constraints on the quality and quantity of data to ensure that the classifications model training can complete in a reasonable amount of time and also meets various performance criteria. These are listed below. Note that none
            are hard restrictions. However, the further one deviates from these guidelines, the greater the chance that the model fails to train or the model will not be satisfactory.</p>
          <ul>
            <li>
              <p>Data quantity</p>
              <ul>
                <li>The highest number of classes classification model has been tested on is ~1200.</li>
                <li>The best suited text size for training and testing data for classification is around 3000 code points. However, larger texts can also be processed, but the runtime performance might be slower.</li>
                <li>Training time will increase based on the number of examples and number of labels.</li>
                <li>Inference time will increased based on the number of labels.</li>
              </ul>
            </li>
            <li>
              <p>Data quality</p>
              <ul>
                <li>Size of each sample (for example, number of phrases in each training sample) can affect quality.</li>
                <li>Class separation is important. In other words, classes among the training (and test) data should be semantically distinguishable from each another in order to avoid misclassifications. Since the classifier algorithms in Watson Natural
                  Language Processing rely on word embeddings, training classes that contain text examples with too much semantic overlap may make high-quality classification computationally intractable. While more sophisticated heuristics may exist for
                  assessing the semantic similarity between classes, you should start with a simple "eye test" of a few examples from each class to discern whether or not they seem adequately separated.</li>
                <li>It is recommended to use balanced data for training. Ideally there should be roughly equal numbers of examples from each class in the training data, otherwise the classifiers may be biased towards classes with larger representation in
                  the training data.</li>
                <li>It is best to avoid circumstances where some classes in the training data are highly under-represented as compared to other classes.</li>
              </ul>
            </li>
          </ul>
          <p>Limitations and caveats:</p>
          <ul>
            <li>The BERT classification block has a predefined sequence length of 128 code points. However, this can be configured at train time by changing the parameter <code>max_seq_length</code>. The maximum value allowed for this parameter is 512. This
              means that the BERT classification block can only be used to classify short text. Text longer than <code>max_seq_length</code> is trimmed and discarded during classification training and inference.</li>
            <li>The CNN classification block has a predefined sequence length of 1000 code points. This limit can be configured at train time by changing the parameter <code>max_phrase_len</code>. There is no maximum limit for this parameter, but increasing
              the maximum phrase length will affect CPU and memory consumption.</li>
            <li>SVM blocks do not have such limit on sequence length and can be used with longer texts.</li>
          </ul>
        </section>
        <section id="section-apply-model">
          <h2 id="apply-model">Applying the model on new data</h2>
          <p>After you have trained the model on a data set, apply the model on new data using the <code>run()</code> method, as you would use on any of the existing pre-trained blocks.</p>
          <p><strong>Sample code</strong></p>
          <ul>
            <li>
              <p>For the Ensemble and BERT models, for example for Ensemble:</p>
              <pre class="codeblock"><code class="lang-python hljs"><span class="hljs-comment"># run Ensemble model on new text</span>
ensemble_prediction = ensemble_classification_model.run(<span class="hljs-string">"new input text"</span>)
</code></pre>
            </li>
            <li>
              <p>For SVM and CNN models, for example for CNN:</p>
              <pre class="codeblock"><code class="lang-python hljs"><span class="hljs-comment"># run Syntax model first</span>
syntax_result = syntax_model.run(<span class="hljs-string">"new input text"</span>)
<span class="hljs-comment"># run CNN model on top of syntax result</span>
cnn_prediction = cnn_classification_model.run(syntax_result)
</code></pre>
            </li>
          </ul>
        </section>
        <section id="section-choose-algorithm">
          <h2 id="choose-algorithm">Choosing the right algorithm for your use case</h2>
          <p>You need to choose the model algorithm that best suits your use case.</p>
          <p>When choosing between SVM, CNN, and Transformers, consider the following:</p>
          <ul>
            <li>
              <p>BERT and Transformer-based Slate</p>
              <ul>
                <li>Choose when high quality is required and higher computing resources are available.</li>
              </ul>
            </li>
            <li>
              <p>CNN</p>
              <ul>
                <li>Choose when decent size data is available</li>
                <li>Choose if GloVe embedding is available for the required language</li>
                <li>Choose if you have the option between single label versus multi-label</li>
                <li>CNN fine tunes embeddings, so it could give better performance for unknown terms or newer domains.</li>
              </ul>
            </li>
            <li>
              <p>SVM</p>
              <ul>
                <li>Choose if an easier and simpler model is required</li>
                <li>SVM has the fastest training and inference time</li>
                <li>Choose if your data set size is small</li>
              </ul>
            </li>
          </ul>
          <p>If you select SVM, you need to consider the following when choosing between the various implementations of SVM:</p>
          <ul>
            <li>SVMs train multi-label classifiers.</li>
            <li>The larger the number of classes, the longer the training time.</li>
            <li>TF-IDF:
              <ul>
                <li>Choose TF-IDF vectorization with SVM if the data set is small, i.e. has a small number of classes, a small number of examples and shorter text size, for example, sentences containing fewer phrases.</li>
                <li>TF-IDF with SVM can be faster than other algorithms in the classification block.</li>
                <li>Choose TF-IDF if embeddings for the required language are not available.</li>
              </ul>
            </li>
            <li>USE:
              <ul>
                <li>Choose Universal Sentence Encoder (USE) with SVM if the data set has one or more sentences in input text.</li>
                <li>USE can perform better on data sets where understanding the context of words or sentences is important.</li>
              </ul>
            </li>
          </ul>
          <p>The Ensemble model combines multiple individual (diverse) models together to deliver superior prediction power. Consider the following key data for this model type:</p>
          <ul>
            <li>The ensemble model combines CNN, SVM with TF-IDF and SVM with USE.</li>
            <li>It is the easiest model to use.</li>
            <li>It can give better performance than the individual algorithms.</li>
            <li>It works for all kinds of data sets. However, training time for large datasets (more than 20000 examples) can be high.</li>
            <li>An ensemble model allows you to set weights. These weights decides how the ensemble model combines the results of individual classifiers. Currently, the selection of weights is a heuristics and needs to be set by trial and error. The default
              weights that are provided in the function itself are a good starting point for the exploration.</li>
          </ul>
          <p><strong>Parent topic:</strong> <a href="watson-nlp-create-model.html">Creating your own models</a></p>
        </section>
      </section>
    </article>
  </main>

<script type="text/javascript"  src="/DEWsG3/JOI980/yQyM/iRupUf/mVarA/YDJa4kXtLhb7/LwkITClvAg/Wgkg/WmBVIDs"></script><link rel="stylesheet" type="text/css"  href="/_sec/cp_challenge/sec-4-4.css">
                                        <script  src="/_sec/cp_challenge/sec-cpt-4-4.js" async defer></script>
                                        <div id="sec-overlay" style="display:none;">
                                        <div id="sec-container">
                                        </div>
                                      </div></body></html>