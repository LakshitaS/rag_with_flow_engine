<!DOCTYPE html><html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="DC.Publisher" content="IBM Corporation">
  <meta name="DC.rights" content="© Copyright IBM Corporation {{ copyright.years }}">
  <meta name="IBM.Country" content="ZZ">
  <meta name="DC.Date" content="{{ lastupdated }}">
  <meta name="keywords" content="{{ keywords }}">
  <meta name="subcollection" content="{{ subcollection }}">
  <meta name="description" content="Try these samples to learn how different prompts can guide foundation models to do common tasks.">
  <meta name="content.type" content="topic">
  <meta name="tags" content="{{ services }}">
  <meta name="account.plan" content="{{ account-plan }}">
  <meta name="completion.time" content="{{ completion-time }}">
  <meta name="version" content="{{ version }}">
  <meta name="deployment.url" content="{{ deployment-url }}">
  <meta name="industry" content="{{ industry }}">
  <meta name="compliance" content="{{ compliance }}">
  <meta name="use.case" content="{{ use-case }}">
  <meta name="source.format" content="markdown">




  <!-- Licensed Materials - Property of IBM -->
  <!-- US Government Users Restricted Rights -->
  <!-- Use, duplication or disclosure restricted by -->
  <!-- GSA ADP Schedule Contract with IBM Corp. -->

  <title>Sample foundation model prompts for common tasks</title>
<link rel="canonical" href="https://www.ibm.com/docs/en/watsonx-as-a-service?topic=lab-sample-prompts"><meta name="viewport" content="width=device-width,initial-scale=1"></head>

<body>
  <main role="main">
    <article aria-labelledby="sample-foundation-model-prompts-for-common-tasks" role="article">
      <style>
        .midd::after {
          content: "\A\00A0\00A0";
          white-space: pre
        }
      </style>
      <section id="section-sample-foundation-model-prompts-for-common-tasks">
        <h1 id="sample-foundation-model-prompts-for-common-tasks">Sample foundation model prompts for common tasks</h1>
        <p>Try these samples to learn how different prompts can guide foundation models to do common tasks.</p>
        <p>&nbsp;</p>
        <section id="section-how-to-use-this-topic">
          <h2 id="how-to-use-this-topic">How to use this topic</h2>
          <p>Explore the sample prompts in this topic:</p>
          <ul>
            <li>Copy and paste the prompt text and input parameter values into the Prompt Lab in IBM watsonx.ai</li>
            <li>See what text is generated.</li>
            <li>See how different models generate different output.</li>
            <li>Change the prompt text and parameters to see how results vary.</li>
          </ul>
          <p>There is no one right way to prompt foundation models. But patterns have been found, in academia and industry, that work fairly reliably. Use the samples in this topic to build your skills and your intuition about prompt engineering through
            experimentation.</p>
          <p>&nbsp;</p>
          <p style="font-size:smaller">This video provides a visual method to learn the concepts and tasks in this documentation.</p>
          <iframe id="prompt-tips" src="https://video.ibm.com/embed/channel/23952663/video/prompt-tips" onload="executeEmbedScript()" lang="en-US" style="border: 0;" webkitallowfullscreen="" allowfullscreen="" frameborder="no" width="560" height="315" title="This video demonstrates concepts to support the samples in this topic.">
</iframe>
          <style type="text/css" keep="true">
            .collapsibleList li>input+* {
              display: none;
            }
            
            .collapsibleList li>input:checked+* {
              display: block;
            }
            
            .collapsibleList li>input {
              display: none;
            }
            
            .collapsibleList label {
              cursor: pointer;
              color: #0062ff;
              text-decoration: underline;
            }
            
            .scrolling_table_div {
              max-height: 300px;
              overflow-y: auto;
              padding-top: 20px;
              margin-bottom: 30px;
            }
            
            .transcript_sync_cb {
              margin: 20px;
            }
            
            .transcript_tbl {
              border-collapse: collapse;
              border: none;
            }
            
            .transcript_tbl th,
            .transcript_tbl td {
              border: none;
            }
            
            .clickable {
              cursor: pointer;
              color: #0062ff;
              vertical-align: top;
            }
            
            .clickable:hover {
              text-decoration: underline;
            }
          </style>
          <script keep="true" src="https://unpkg.com/ibm-video-streaming-web-player-api@1.1.0/dist/index.umd.min.js"></script>
          <script keep="true">
            var g_viewer = null;
            var g_b_is_playing = false;
            var g_b_do_retry = true;
            var g_highlighted_row_num = -1;
            var g_org_color = "";

            var g_loop = setInterval(function() {
              if (g_viewer && document.getElementById("sync_cb_prompt-tips").checked) {
                synchronize();
              }

            }, 2 * 1000);

            function executeEmbedScript() {
              if (g_viewer) {
                // Chrome is running this function twice??
                return;
              }

              g_viewer = PlayerAPI("prompt-tips");

              var url = window.location.href;
              if (!url.match(/t\=\d+/i)) {
                return;
              }

              g_viewer.addListener("playing", function(event_key, b_is_playing) {
                g_b_is_playing = b_is_playing;

                if (b_is_playing) {
                  g_b_do_retry = false;
                }

              });

              document.getElementById("transcript_btn_prompt-tips").click();
              var seconds = url.replace(/^.*t\=(\d+).*$/i, "$1");
              synchronize(seconds);
              jumpToTimeRetry(seconds, 5);
            }

            function jumpTo(obj) {
              var seconds = obj.parentNode.id;
              jumpToTime(seconds);
            }

            function jumpToTime(seconds) {
              g_viewer.callMethod("seek", seconds);
              g_viewer.callMethod("play");
            }

            function jumpToTimeRetry(seconds, iteration) {
              if (iteration < 1) {
                // Give up
                return;
              }

              g_viewer.callMethod("seek", seconds);
              g_viewer.callMethod("play");

              setTimeout(function() {
                // Wait 1.5 seconds and then try again
                if (g_b_do_retry) {
                  jumpToTimeRetry(seconds, iteration - 1);
                }

              }, 1.5 * 1000);
            }

            function synchronize(seconds_in) {
              if (seconds_in) {
                scrollAndHighlight(seconds_in);
                return;
              }

              g_viewer.getProperty("progress", function(seconds) {
                scrollAndHighlight(seconds);

              });

            }

            function scrollAndHighlight(seconds) {
              var rows_arr = document.getElementsByClassName("video_tr");
              for (var i = (rows_arr.length - 1); i >= 0; i--) {
                if (parseInt(rows_arr[i].id) > seconds) {
                  continue;
                }

                if (i == g_highlighted_row_num) {
                  break;
                }

                if (-1 !== g_highlighted_row_num) {
                  unhighlight(rows_arr[g_highlighted_row_num]);
                }

                g_highlighted_row_num = i;
                highlight(rows_arr[g_highlighted_row_num]);
                scrollToRow(rows_arr[g_highlighted_row_num]);

                break;
              }

            }

            function scrollToRow(row_obj) {
              var row_top = row_obj.offsetTop;
              var row_bottom = row_top + row_obj.clientHeight;

              var scroller = document.getElementById("transcript_div_prompt-tips");
              var scroll_top = scroller.scrollTop;
              var scroll_bottom = scroll_top + scroller.clientHeight;

              if ((row_top < scroll_top) && (row_bottom > scroll_bottom)) {
                // Edge case: Row taller than div
                // Don't do anything
              } else if ((row_top < scroll_top) || (row_bottom > scroll_bottom)) {
                scroller.scrollTop = row_top;
              }
            }

            function highlight(row_obj) {
              var obj_arr = row_obj.childNodes;
              for (var i = 0; i < obj_arr.length; i++) {
                if ("TD" == obj_arr[i].nodeName) {
                  g_org_color = obj_arr[i].style.backgroundColor;
                  obj_arr[i].style.backgroundColor = "#c6c6c6";
                }
              }
            }

            function unhighlight(row_obj) {
              var obj_arr = row_obj.childNodes;
              for (var i = 0; i < obj_arr.length; i++) {
                if ("TD" == obj_arr[i].nodeName) {
                  obj_arr[i].style.backgroundColor = g_org_color;
                }
              }
            }

            function toggleHighlighting() {
              if (!document.getElementById("sync_cb_prompt-tips").checked && (-1 !== g_highlighted_row_num)) {
                var rows_arr = document.getElementsByClassName("video_tr");
                unhighlight(rows_arr[g_highlighted_row_num]);
                g_highlighted_row_num = -1;
              }
            }
          </script>
          <p><br><b>Video chapters</b><br> [ <span class="clickable" onclick="jumpToTime(  11 );" tabindex="0" role="button" title="Play video from this time">0:11</span> ] Introduction to prompts and Prompt Lab<br> [ <span class="clickable" onclick="jumpToTime(  33 );" tabindex="0" role="button" title="Play video from this time">0:33</span> ] Key concept: Everything is text completion<br> [ <span class="clickable" onclick="jumpToTime(  94 );" tabindex="0" role="button" title="Play video from this time">1:34</span>            ] Useful prompt pattern: Few-shot prompt<br> [ <span class="clickable" onclick="jumpToTime( 118 );" tabindex="0" role="button" title="Play video from this time">1:58</span> ] Stopping criteria: Max tokens, stop sequences<br> [ <span class="clickable" onclick="jumpToTime( 212 );" tabindex="0" role="button" title="Play video from this time">3:32</span> ] Key concept: Fine-tuning<br> [ <span class="clickable" onclick="jumpToTime( 272 );" tabindex="0" role="button" title="Play video from this time">4:32</span>            ] Useful prompt pattern: Zero-shot prompt<br> [ <span class="clickable" onclick="jumpToTime( 332 );" tabindex="0" role="button" title="Play video from this time">5:32</span> ] Key concept: Be flexible, try different prompts<br> [ <span class="clickable" onclick="jumpToTime( 374 );" tabindex="0" role="button" title="Play video from this time">6:14</span> ] Next steps: Experiment with sample prompts<br></p>
          <p>&nbsp;</p>
        </section>
        <section id="section-samples-overview">
          <h2 id="samples-overview">Samples overview</h2>
          <p>You can find samples that prompt foundation models to generate output that supports the following tasks:</p>
          <ul>
            <li><a href="#classification">Classification</a></li>
            <li><a href="#extraction">Extraction</a></li>
            <li><a href="#generation">Generation</a></li>
            <li><a href="#qa">Question answering (QA)</a></li>
            <li><a href="#summarization">Summarization</a></li>
            <li><a href="#code">Code generation and conversion</a></li>
            <li><a href="#dialog">Dialog</a></li>
            <li><a href="#translation">Translation</a></li>
          </ul>
          <p>The following table shows the foundation models that are used in task-specific samples. A checkmark (✓) indicates that the model is used in a sample for the associated task. You can click <strong>See sample</strong> to go to the sample prompt.</p>
          <table>
            <caption caption-side="top">Table 1. Models used in samples for certain tasks</caption>
            <thead>
              <tr>
                <th>Model</th>
                <th>Classification</th>
                <th>Extraction</th>
                <th>Generation</th>
                <th>QA</th>
                <th>Summarization</th>
                <th>Coding</th>
                <th>Dialog</th>
                <th>Translation</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>granite-13b-chat-v2</td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td>✓<br><a href="#sample7c">See sample</a></td>
                <td></td>
              </tr>
              <tr>
                <td>granite-13b-instruct-v2</td>
                <td></td>
                <td>✓<br><a href="#sample2a">See sample</a></td>
                <td>✓<br><a href="#sample3c">See sample</a></td>
                <td>✓<br><a href="#sample4c">See sample</a></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td>granite-8b-japanese</td>
                <td></td>
                <td></td>
                <td></td>
                <td>✓<br><a href="#sample4e">See sample</a></td>
                <td></td>
                <td></td>
                <td>✓<br><a href="#sample7d">See sample</a></td>
                <td>✓<br><a href="#sample8c">See sample</a></td>
              </tr>
              <tr>
                <td>granite-20b-multilingual</td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td>✓<br><a href="#sample8d">See sample</a></td>
              </tr>
              <tr>
                <td>codellama-34b-instruct-hf</td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td>✓<br><a href="#sample6a">See sample</a></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td>elyza-japanese-llama-2-7b-instruct</td>
                <td>✓<br><a href="#sample1d">See sample</a></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td>✓<br><a href="#sample8a">See sample</a></td>
              </tr>
              <tr>
                <td>flan-t5-xxl-11b</td>
                <td>✓<br><a href="#sample1a">See sample</a></td>
                <td></td>
                <td></td>
                <td>✓<br><a href="#sample4a">See sample</a></td>
                <td>✓<br><a href="#sample5a">See sample</a></td>
                <td></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td>flan-ul2-20b</td>
                <td>✓<br><a href="#sample1a">See sample</a></td>
                <td>✓<br><a href="#sample2a">See sample</a></td>
                <td></td>
                <td>✓<br><a href="#sample4b">See sample</a></td>
                <td>✓<br><a href="#sample4a">See sample</a></td>
                <td>✓<br><a href="#sample5a">See sample</a></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td>jais-13b-chat</td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td>✓<br><a href="#sample7e">See sample</a></td>
                <td></td>
              </tr>
              <tr>
                <td>llama-2 chat</td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td>✓<br><a href="#sample7a">See sample</a></td>
                <td></td>
              </tr>
              <tr>
                <td>llama2-13b-dpo-v7</td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td>✓<br><a href="#sample5d">See sample</a></td>
                <td></td>
                <td>✓<br><a href="#sample7f">See sample</a></td>
                <td></td>
              </tr>
              <tr>
                <td>mixtral-8x7b-instruct-v01</td>
                <td>✓<br><a href="#sample1b">See sample</a></td>
                <td></td>
                <td>✓<br><a href="#sample3a">See sample</a></td>
                <td>✓<br><a href="#sample4a">See sample</a></td>
                <td>✓<br><a href="#sample5b">See sample</a></td>
                <td>✓<br><a href="#sample6a">See sample</a></td>
                <td></td>
                <td>✓<br><a href="#sample8b">See sample</a></td>
              </tr>
              <tr>
                <td>mt0-xxl-13b</td>
                <td>✓<br><a href="#sample1a">See sample</a></td>
                <td></td>
                <td></td>
                <td>✓<br><a href="#sample4a">See sample</a></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td>starcoder-15.5b</td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
                <td>✓<br><a href="#sample6a">See sample</a></td>
                <td></td>
                <td></td>
              </tr>
            </tbody>
          </table>
          <p>The following table summarizes the available sample prompts.</p>
          <table>
            <caption caption-side="top">Table 2. List of sample prompts</caption>
            <thead>
              <tr>
                <th>Scenario</th>
                <th>Prompt editor</th>
                <th>Prompt format</th>
                <th>Model</th>
                <th>Decoding</th>
                <th>Notes</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><a href="#sample1a">Sample with a zero-shot prompt: Classify a message</a></td>
                <td>Freeform</td>
                <td>Zero-shot</td>
                <td>• mt0-xxl-13b<br> • flan-t5-xxl-11b<br> • flan-ul2-20b<br> • mixtral-8x7b-instruct-v01</td>
                <td>Greedy</td>
                <td>• Uses the class names as stop sequences to stop the model after it prints the class name</td>
              </tr>
              <tr>
                <td><a href="#sample1b">Sample with a few-shot prompt: Classify a message in freeform mode</a></td>
                <td>Freeform</td>
                <td>Few-shot</td>
                <td>• mixtral-8x7b-instruct-v01</td>
                <td>Greedy</td>
                <td>• Uses the class names as stop sequences</td>
              </tr>
              <tr>
                <td><a href="#sample1c">Sample with a few-shot prompt: Classify a message in structured mode</a></td>
                <td>Structured</td>
                <td>Few-shot</td>
                <td>• mixtral-8x7b-instruct-v01</td>
                <td>Greedy</td>
                <td>• Uses the class names as stop sequences</td>
              </tr>
              <tr>
                <td><a href="#sample1d">Sample: Classify a Japanese message</a></td>
                <td>Freeform</td>
                <td>Few-shot</td>
                <td>• elyza-japanese-llama-2-7b-instruct</td>
                <td>Greedy</td>
                <td>• Uses the class names as stop sequences</td>
              </tr>
              <tr>
                <td><a href="#sample2a">Sample: Extract details from a complaint</a></td>
                <td>Freeform</td>
                <td>Zero-shot</td>
                <td>• flan-ul2-20b</td>
                <td>Greedy</td>
                <td></td>
              </tr>
              <tr>
                <td><a href="#sample3a">Sample: Generate a numbered list on a theme in freeform mode</a></td>
                <td>Freeform</td>
                <td>Few-shot</td>
                <td>• mixtral-8x7b-instruct-v01</td>
                <td>Sampling</td>
                <td>• Generates formatted output<br> • Uses two newline characters as a stop sequence to stop the model after one list</td>
              </tr>
              <tr>
                <td><a href="#sample3b">Sample: Generate a numbered list on a theme in structured mode</a></td>
                <td>Structured</td>
                <td>Few-shot</td>
                <td>• mixtral-8x7b-instruct-v01</td>
                <td>Sampling</td>
                <td>• Generates formatted output.<br>• Uses two newline characters as a stop sequence</td>
              </tr>
              <tr>
                <td><a href="#sample3c">Sample: Generate a numbered list on a particular theme with Granite</a></td>
                <td>Freeform</td>
                <td>Zero-shot</td>
                <td>• granite-13b-instruct-v2</td>
                <td>Greedy</td>
                <td>• Generates formatted output</td>
              </tr>
              <tr>
                <td><a href="#sample4a">Sample: Answer a question based on an article in freeform mode</a></td>
                <td>Freeform</td>
                <td>Zero-shot</td>
                <td>• mt0-xxl-13b<br> • flan-t5-xxl-11b<br> • flan-ul2-20b<br> • mixtral-8x7b-instruct-v01</td>
                <td>Greedy</td>
                <td>• Uses a period "." as a stop sequence to cause the model to return only a single sentence</td>
              </tr>
              <tr>
                <td><a href="#sample4b">Sample: Answer a question based on an article in structured mode</a></td>
                <td>Structured</td>
                <td>Zero-shot</td>
                <td>• mt0-xxl-13b<br> • flan-t5-xxl-11b<br> • flan-ul2-20b<br> • mixtral-8x7b-instruct-v01</td>
                <td>Greedy</td>
                <td>• Uses a period "." as a stop sequence<br>• Generates results for multiple inputs at once</td>
              </tr>
              <tr>
                <td><a href="#sample4c">Sample: Answer a question based on a document with Granite</a></td>
                <td>Freeform</td>
                <td>Zero-shot</td>
                <td>• granite-13b-instruct-v2</td>
                <td>Greedy</td>
                <td></td>
              </tr>
              <tr>
                <td><a href="#sample4e">Sample: Answer general knowledge questions in Japanese</a></td>
                <td>Freeform</td>
                <td>Zero-shot</td>
                <td>• granite-8b-japanese</td>
                <td>Greedy</td>
                <td></td>
              </tr>
              <tr>
                <td><a href="#sample5a">Sample zero-shot prompt: Summarize a meeting transcript</a></td>
                <td>Freeform</td>
                <td>Zero-shot</td>
                <td>• flan-t5-xxl-11b<br> • flan-ul2-20b<br> • mixtral-8x7b-instruct-v01</td>
                <td>Greedy</td>
                <td></td>
              </tr>
              <tr>
                <td><a href="#sample5b">Sample few-shot prompt: Summarize a meeting transcript in freeform mode</a></td>
                <td>Freeform</td>
                <td>Few-shot</td>
                <td>• mixtral-8x7b-instruct-v01</td>
                <td>Greedy</td>
                <td></td>
              </tr>
              <tr>
                <td><a href="#sample5c">Sample few-shot prompt: Summarize a meeting transcript in structured mode</a></td>
                <td>Structured</td>
                <td>Few-shot</td>
                <td>• mixtral-8x7b-instruct-v01</td>
                <td>Greedy</td>
                <td>• Generates formatted output<br>• Uses two newline characters as a stop sequence to stop the model after one list</td>
              </tr>
              <tr>
                <td><a href="#sample5d">Sample: Summarize content in Korean</a></td>
                <td>Freeform</td>
                <td>Zero-shot</td>
                <td>• llama2-13b-dpo-v7</td>
                <td>Greedy</td>
                <td>• Generates output in Korean</td>
              </tr>
              <tr>
                <td><a href="#sample6a">Sample: Generate programmatic code from instructions</a></td>
                <td>Freeform</td>
                <td>Few-shot</td>
                <td>• starcoder-15.5b<br> • mixtral-8x7b-instruct-v01<br> • codellama-34b-instruct-hf</td>
                <td>Greedy</td>
                <td>• Generates programmatic code as output<br>• Uses &lt;end of code&gt; as a stop sequence</td>
              </tr>
              <tr>
                <td><a href="#sample6b">Sample: Convert code from one programming language to another</a></td>
                <td>Freeform</td>
                <td>Few-shot</td>
                <td>• starcoder-15.5b<br> • mixtral-8x7b-instruct-v01<br> • codellama-34b-instruct-hf</td>
                <td>Greedy</td>
                <td>• Generates programmatic code as output<br>• Uses &lt;end of code&gt; as a stop sequence</td>
              </tr>
              <tr>
                <td><a href="#sample7a">Sample: Converse with llama-2 chat</a></td>
                <td>Freeform</td>
                <td>Custom structure</td>
                <td>• llama-2 chat</td>
                <td>Greedy</td>
                <td>• Generates dialog output like a chatbot<br>• Uses a model-specific prompt format</td>
              </tr>
              <tr>
                <td><a href="#sample7b">Sample: Converse in a dialog</a></td>
                <td>Freeform</td>
                <td>Custom structure</td>
                <td>• granite-13b-chat-v1</td>
                <td>Greedy</td>
                <td>• Generates dialog output like a chatbot<br>• Uses a special token that is named END_KEY as a stop sequence</td>
              </tr>
              <tr>
                <td><a href="#sample7c">Sample: Converse with granite-13b-chat-v2</a></td>
                <td>Freeform</td>
                <td>Custom structure</td>
                <td>• granite-13b-chat-v2</td>
                <td>Greedy</td>
                <td>• Generates dialog output like a chatbot<br>• Uses a system prompt to establish guardrails for the dialog</td>
              </tr>
              <tr>
                <td><a href="#sample7d">Sample: Converse in Japanese with granite-8b-japanese</a></td>
                <td>Freeform</td>
                <td>Custom structure</td>
                <td>• granite-8b-japanese</td>
                <td>Greedy</td>
                <td>• Generates Japanese dialog output like a chatbot<br>• Uses a model-specific prompt format</td>
              </tr>
              <tr>
                <td><a href="#sample7e">Sample: Converse in Arabic with jais-13b-chat</a></td>
                <td>Freeform</td>
                <td>Custom structure</td>
                <td>• jais-13b-chat</td>
                <td>Greedy</td>
                <td>• Generates English or Arabic dialog output like a chatbot<br>• Uses a model-specific prompt format</td>
              </tr>
              <tr>
                <td><a href="#sample7f">Sample: Converse in Korean with llama2-13b-dpo-v7</a></td>
                <td>Freeform</td>
                <td>Custom structure</td>
                <td>• llama2-13b-dpo-v7</td>
                <td>Greedy</td>
                <td>• Generates Korean dialog output like a chatbot<br>• Uses a model-specific prompt format<br>• Uses a period as a stop sequence</td>
              </tr>
              <tr>
                <td><a href="#sample8a">Sample: Translate text from Japanese to English</a></td>
                <td>Freeform</td>
                <td>Zero-shot</td>
                <td>• elyza-japanese-llama-2-7b-instruct</td>
                <td>Greedy</td>
                <td>• Translates text from Japanese to English</td>
              </tr>
              <tr>
                <td><a href="#sample8b">Sample: Translate text from Spanish to English</a></td>
                <td>Freeform</td>
                <td>Few-shot</td>
                <td>• mixtral-8x7b-instruct-v01</td>
                <td>Greedy</td>
                <td>• Translates text from Spanish to English</td>
              </tr>
              <tr>
                <td><a href="#sample8c">Sample: Translate text from English to Japanese</a></td>
                <td>Freeform</td>
                <td>Zero-shot</td>
                <td>• granite-8b-japanese</td>
                <td>Greedy</td>
                <td>• Translates text from English to Japanese</td>
              </tr>
              <tr>
                <td><a href="#sample8d">Sample: Translate text from French to English</a></td>
                <td>Freeform</td>
                <td>Few-shot</td>
                <td>• granite-20b-multilingual</td>
                <td>Greedy</td>
                <td>• Translates text from French to English</td>
              </tr>
            </tbody>
          </table>
        </section>
        <section id="section-classification">
          <h2 id="classification">Classification</h2>
          <p>Classification is useful for predicting data in distinct categories. Classifications can be binary, with two classes of data, or multi-class. A classification task is useful for categorizing information, such as customer feedback, so that you
            can manage or act on the information more efficiently.</p>
          <section id="section-sample1a">
            <h3 id="sample1a">Sample with a zero-shot prompt: Classify a message</h3>
            <p>Scenario: Given a message that is submitted to a customer-support chatbot for a cloud software company, classify the customer's message as either a question or a problem. Depending on the class assignment, the chat is routed to the correct
              support team for the issue type.</p>
            <p><strong>Model choice</strong><br> Models that are instruction-tuned can generally complete this task with this sample prompt.</p>
            <p>Suggestions: mt0-xxl-13b, flan-t5-xxl-11b, flan-ul2-20b, or mixtral-8x7b-instruct-v01</p>
            <p><strong>Decoding</strong><br> Greedy. The model must return one of the specified class names; it cannot be creative and make up new classes.</p>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>Specify two stop sequences: "Question" and "Problem". After the model generates either of those words, it should stop.</li>
              <li>With such short output, the Max tokens parameter can be set to 5.</li>
            </ul>
            <p><strong>Prompt text</strong><br> Paste this zero-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">Classify this customer message into one of two classes: Question, Problem.

Class name: Question
Description: The customer is asking a technical question or a how-to question 
about our products or services.

Class name: Problem
Description: The customer is describing a problem they are having. They might 
say they are trying something, but it's not working. They might say they are 
getting an error or unexpected results.

Message: I'm having trouble registering for a new account.
Class name: 
</code></pre>
          </section>
          <section id="section-sample1b">
            <h3 id="sample1b">Sample with a few-shot prompt: Classify a message in freeform mode</h3>
            <p>Scenario: Given a message that is submitted to a customer-support chatbot for a cloud software company, classify the customer's message as either a question or a problem description. Based on the class type, the chat can be routed to the correct
              support team.</p>
            <p><strong>Model choice</strong><br> With few-shot examples of both classes, models such as mixtral-8x7b-instruct-v01 can complete this task well.</p>
            <p><strong>Decoding</strong><br> Greedy. The model must return one of the specified class names; it cannot be creative and make up new classes.</p>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>Specify two stop sequences: "Question" and "Problem". After the model generates either of those words, it should stop.</li>
              <li>With such short output, the Max tokens parameter can be set to 5.</li>
            </ul>
            <p><strong>Prompt text</strong><br> Paste this few-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">Message: When I try to log in, I get an error.
Class name: Problem

Message: Where can I find the plan prices?
Class name: Question

Message: What is the difference between trial and paygo?
Class name: Question

Message: The registration page crashed, and now I can't create a new account.
Class name: Problem

Message: What regions are supported?
Class name: Question

Message: I can't remember my password.
Class name: Problem

Message: I'm having trouble registering for a new account.
</code></pre>
          </section>
          <section id="section-sample1c">
            <h3 id="sample1c">Sample with a few-shot prompt: Classify a message in structured mode</h3>
            <p>Scenario: Given a message that is submitted to a customer-support chatbot for a cloud software company, classify the customer's message as either a question or a problem description. Based on the class type, the chat can be routed to the correct
              support team.</p>
            <p><strong>Model choice</strong><br> With few-shot examples of both classes, models such as mixtral-8x7b-instruct-v01 can complete this task well.</p>
            <p><strong>Decoding</strong><br> Greedy. The model must return one of the specified class names, not be creative and make up new classes.</p>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>Specify two stop sequences: "Question" and "Problem". After the model generates either of those words, it should stop.</li>
              <li>With such short output, the Max tokens parameter can be set to 5.</li>
            </ul>
            <p><strong>Set up section</strong><br> Paste these headers and examples into the <strong>Examples</strong> area of the <strong>Set up</strong> section:</p>
            <table>
              <caption caption-side="top">Table 2. Classification few-shot examples</caption>
              <tbody><tr>
                <th><b>Message:</b></th>
                <th><b>Class name:</b></th>
              </tr>
              <tr>
                <td valign="top"><code>When I try to log in, I get an error.</code></td>
                <td valign="top"><code>Problem</code></td>
              </tr>
              <tr>
                <td valign="top"><code>Where can I find the plan prices?</code></td>
                <td valign="top"><code>Question</code></td>
              </tr>
              <tr>
                <td valign="top"><code>What is the difference between trial and paygo?</code></td>
                <td valign="top"><code>Question</code></td>
              </tr>
              <tr>
                <td valign="top"><code>The registration page crashed, and now I can't create a new account.</code></td>
                <td valign="top"><code>Problem</code></td>
              </tr>
              <tr>
                <td valign="top"><code>What regions are supported?</code></td>
                <td valign="top"><code>Question</code></td>
              </tr>
              <tr>
                <td valign="top"><code>I can't remember my password.</code></td>
                <td valign="top"><code>Problem</code></td>
              </tr>
            </tbody></table>
            <p><br><strong>Try section</strong><br> Paste this message in the <strong>Try</strong> section:</p>
            <pre class="codeblock"><code class="lang-txt hljs">I'm having trouble registering for a new account.
</code></pre>
            <p>Select the model and set parameters, then click <strong>Generate</strong> to see the result.</p>
          </section>
          <section id="section-sample1d">
            <h3 id="sample1d">Sample: Classify a Japanese message</h3>
            <p>Scenario: Given a message that is submitted to a customer-support chatbot for a Japanese cloud software company, classify the customer's message as either a question or a problem description. Based on the class type, the chat can be routed
              to the correct support team.</p>
            <p><strong>Model choice</strong><br> The elyza-japanese-llama-2-7b-instruct model can classify prompt input text that is written in Japanese.</p>
            <p><strong>AI guardrails</strong><br> Disable the <strong>AI guardrails</strong> feature. The feature is supported with English text only. It might flag as inappropriate content that is not inappropriate.</p>
            <p><strong>Decoding</strong><br> Greedy. The model must return one of the specified class names; it cannot be creative and make up new classes.</p>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>Specify two stop sequences: <code>問題</code> for problem and <code>質問</code> for question. After the model generates either of those words, it should stop.</li>
              <li>If you want to lower the value in the Max tokens parameter, do not lower the value below 7 tokens. Japanese characters use more tokens than the same words in English.</li>
            </ul>
            <p><strong>Prompt text</strong><br> Paste this few-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result. The sample prompt text is a Japanese
              translation of the English prompt text in <a href="#sample1b">Sample 1b</a>.</p>
            <pre class="codeblock"><code class="lang-txt hljs">次のメッセージを問題または質問に分類します。

メッセージ: ログインしようとすると、エラーが発生します。
クラス名: 問題

メッセージ: プランの価格はどこで確認できますか?
クラス名: 質問

メッセージ: トライアルとペイゴーの違いは何ですか?
クラス名: 質問

メッセージ: 登録ページがクラッシュしたため、新しいアカウントを作成できません。
クラス名: 問題

メッセージ: どの地域がサポートされていますか?
クラス名: 質問

メッセージ: パスワードを思い出せません。
クラス名: 問題

メッセージ: 新しいアカウントの登録で問題が発生しました。
クラス名：
</code></pre>
            <p>&nbsp;</p>
          </section>
        </section>
        <section id="section-extraction">
          <h2 id="extraction">Extracting details</h2>
          <p>Extraction tasks can help you to find key terms or mentions in data based on the semantic meaning of words rather than simple text matches.</p>
          <section id="section-sample2a">
            <h3 id="sample2a">Sample: Extract details from a complaint</h3>
            <p>Scenario: Given a complaint from a customer who had trouble booking a flight on a reservation website, identify the factors that contributed to this customer's unsatisfactory experience.</p>
            <p><strong>Model choices</strong><br> flan-ul2-20b
            </p>
            <p><strong>Decoding</strong><br> Greedy. We need the model to return words that are in the input; the model cannot be creative and make up new words.</p>
            <p><strong>Stopping criteria</strong><br> The list of extracted factors will not be long, so set the Max tokens parameter to 50.</p>
            <p><strong>Prompt text</strong><br> Paste this zero-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">From the following customer complaint, extract all the factors that 
caused the customer to be unhappy.

Customer complaint:
I just tried to book a flight on your incredibly slow website. All 
the times and prices were confusing. I liked being able to compare 
the amenities in economy with business class side by side. But I 
never got to reserve a seat because I didn't understand the seat map. 
Next time, I'll use a travel agent!

Numbered list of all the factors that caused the customer to be unhappy:
</code></pre>
          </section>
        </section>
        <section id="section-generation">
          <h2 id="generation">Generating natural language</h2>
          <p>Generation tasks are what large language models do best. Your prompts can help guide the model to generate useful language.</p>
          <section id="section-sample3a">
            <h3 id="sample3a">Sample with a few-shot prompt: Generate a numbered list on a theme in freeform mode</h3>
            <p>Scenario: Generate a numbered list on a particular theme.</p>
            <p><strong>Model choice</strong><br> The mixtral-8x7b-instruct-v01 foundation model was trained to recognize and handle special characters, such as the newline character, well. This model is a good choice when you want your generated text to
              be formatted a specific way with special characters.</p>
            <p><strong>Decoding</strong><br> Sampling. This is a creative task. Set the following parameters:</p>
            <ul>
              <li>Temperature: 0.7</li>
              <li>Top P: 1</li>
              <li>Top K: 50</li>
              <li>Random seed: 9045 (To get different output each time you click <strong>Generate</strong>, specify a different value for the Random seed parameter or clear the parameter.)</li>
            </ul>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>To make sure the model stops generating text after one list, specify a stop sequence of two newline characters. To do that, click the <strong>Stop sequence</strong> text box, press the Enter key twice, and then click <strong>Add sequence</strong>.</li>
              <li>The list will not be very long, so set the Max tokens parameter to 50.</li>
            </ul>
            <p><strong>Prompt text</strong><br> Paste this few-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">What are 4 types of dog breed?
1. Poodle
2. Dalmatian
3. Golden retriever
4. Bulldog

What are 3 ways to incorporate exercise into your day?
1. Go for a walk at lunch
2. Take the stairs instead of the elevator
3. Park farther away from your destination

What are 4 kinds of vegetable?
1. Spinach
2. Carrots
3. Broccoli
4. Cauliflower

What are the 3 primary colors?
1. Red
2. Green
3. Blue

What are 3 ingredients that are good on pizza?
</code></pre>
          </section>
          <section id="section-sample3b">
            <h3 id="sample3b">Sample with a few-shot prompt: Generate a numbered list on a theme in structured mode</h3>
            <p>Scenario: Generate a numbered list on a particular theme.</p>
            <p><strong>Model choice</strong><br> The mixtral-8x7b-instruct-v01 foundation model was trained to recognize and handle special characters, such as the newline character, well. This model is a good choice when you want your generated text to
              be formatted a specific way with special characters.</p>
            <p><strong>Decoding</strong><br> Sampling. This scenario is a creative one. Set the following parameters:</p>
            <ul>
              <li>Temperature: 0.7</li>
              <li>Top P: 1</li>
              <li>Top K: 50</li>
              <li>Random seed: 9045 (To generate different results, specify a different value for the Random seed parameter or clear the parameter.)</li>
            </ul>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>To make sure that the model stops generating text after one list, specify a stop sequence of two newline characters. To do that, click in the <strong>Stop sequence</strong> text box, press the Enter key twice, and then click <strong>Add sequence</strong>.</li>
              <li>The list will not be long, so set the Max tokens parameter to 50.</li>
            </ul>
            <p><strong>Set up section</strong><br> Paste these headers and examples into the <strong>Examples</strong> area of the <strong>Set up</strong> section:</p>
            <table>
              <caption caption-side="top">Table 3. Generation few-shot examples</caption>
              <tbody><tr>
                <th><b><code>Input:</code></b></th>
                <th><b><code>Output:</code></b></th>
              </tr>
              <tr>
                <td valign="top"><code>What are 4 types of dog breed?</code></td>
                <td valign="top"><code>1. Poodle<br>2. Dalmatian<br>3. Golden retriever<br>4. Bulldog</code></td>
              </tr>
              <tr>
                <td valign="top"><code>What are 3 ways to incorporate exercise into your day?</code></td>
                <td valign="top"><code>1. Go for a walk at lunch<br>2. Take the stairs instead of the elevator<br>3. Park farther away from your destination</code></td>
              </tr>
              <tr>
                <td valign="top"><code>What are 4 kinds of vegetable?</code></td>
                <td valign="top"><code>1. Spinach<br>2. Carrots<br>3. Broccoli<br>4. Cauliflower</code></td>
              </tr>
              <tr>
                <td valign="top"><code>What are the 3 primary colors?</code></td>
                <td valign="top"><code>1. Red<br>2. Green<br>3. Blue</code></td>
              </tr>
            </tbody></table>
            <p><br><strong>Try section</strong><br> Paste this input in the <strong>Try</strong> section:</p>
            <pre class="codeblock"><code class="lang-txt hljs">What are 3 ingredients that are good on pizza?
</code></pre>
            <p>Select the model and set parameters, then click <strong>Generate</strong> to see the result.</p>
          </section>
          <section id="section-sample3c">
            <h3 id="sample3c">Sample with a zero-shot prompt: Generate a numbered list on a particular theme</h3>
            <p>Scenario: Ask the model to play devil's advocate. Describe a potential action and ask the model to list possible downsides or risks that are associated with the action.</p>
            <p><strong>Model choice</strong><br> The granite-13b-instruct model was trained to recognize and handle special characters, such as the newline character, well. The granite-13b-instruct-v2 model is a good choice when you want your generated text
              to be formatted in a specific way with special characters.</p>
            <p><strong>Decoding</strong><br> Greedy. The model must return the most predictable content based on what's in the prompt; the model cannot be too creative.</p>
            <p><strong>Stopping criteria</strong><br> The summary might run several sentences, so set the Max tokens parameter to 60.</p>
            <p><strong>Prompt text</strong><br> Paste this prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">You are playing the role of devil's advocate. Argue against the proposed plans. List 3 detailed, unique, compelling reasons why moving forward with the plan would be a bad choice. Consider all types of risks.

Plan we are considering:
Extend our store hours.
Three problems with this plan are:
1. We'll have to pay more for staffing.
2. Risk of theft increases late at night.
3. Clerks might not want to work later hours.

Plan we are considering:
Open a second location for our business.
Three problems with this plan are:
1. Managing two locations will be more than twice as time-consuming than managed just one.
2. Creating a new location doesn't guarantee twice as many customers.
3. A new location means added real estate, utility, and personnel expenses.

Plan we are considering:
Refreshing our brand image by creating a new logo.
Three problems with this plan are:

</code></pre>
          </section>
        </section>
        <section id="section-qa">
          <h2 id="qa">Question answering</h2>
          <p>Question-answering tasks are useful in help systems and other scenarios where frequently asked or more nuanced questions can be answered from existing content.</p>
          <p>To help the model return factual answers, implement the retrieval-augmented generation pattern. For more information, see <a href="fm-rag.html">Retrieval-augmented generation</a>.</p>
          <section id="section-sample4a">
            <h3 id="sample4a">Sample: Answer a question based on an article in freeform mode</h3>
            <p>Scenario: The website for an online seed catalog has many articles to help customers plan their garden and ultimately select which seeds to purchase. A new widget is being added to the website to answer customer questions based on the contents
              of the article the customer is viewing. Given a question that is related to an article, answer the question based on the article.</p>
            <p><strong>Model choice</strong><br> Models that are instruction-tuned, such as flan-t5-xxl-11b, flan-ul2-20b, mixtral-8x7b-instruct-v01, or mt0-xxl-13b can generally complete this task with this sample prompt.</p>
            <p><strong>Decoding</strong><br> Greedy. The answers must be grounded in the facts in the article, and if there is no good answer in the article, the model should not be creative and make up an answer.</p>
            <p><strong>Stopping criteria</strong><br> To cause the model to return a one-sentence answer, specify a period "." as a stop sequence. The Max tokens parameter can be set to 50.</p>
            <p><strong>Prompt text</strong><br> Paste this zero-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">Article:
###
Tomatoes are one of the most popular plants for vegetable gardens. 
Tip for success: If you select varieties that are resistant to 
disease and pests, growing tomatoes can be quite easy. For 
experienced gardeners looking for a challenge, there are endless 
heirloom and specialty varieties to cultivate. Tomato plants come 
in a range of sizes. There are varieties that stay very small, less 
than 12 inches, and grow well in a pot or hanging basket on a balcony 
or patio. Some grow into bushes that are a few feet high and wide, 
and can be grown is larger containers. Other varieties grow into 
huge bushes that are several feet wide and high in a planter or 
garden bed. Still other varieties grow as long vines, six feet or 
more, and love to climb trellises. Tomato plants do best in full 
sun. You need to water tomatoes deeply and often. Using mulch 
prevents soil-borne disease from splashing up onto the fruit when you 
water. Pruning suckers and even pinching the tips will encourage the 
plant to put all its energy into producing fruit.
###

Answer the following question using only information from the article. 
Answer in a complete sentence, with proper capitalization and punctuation. 
If there is no good answer in the article, say "I don't know".

Question: Why should you use mulch when growing tomatoes?
Answer: 
</code></pre>
            <p>You can experiment with asking other questions too, such as:</p>
            <ul>
              <li>"How large do tomato plants get?"</li>
              <li>"Do tomato plants prefer shade or sun?"</li>
              <li>"Is it easy to grow tomatoes?"</li>
            </ul>
            <p>Try out-of-scope questions too, such as:</p>
            <ul>
              <li>"How do you grow cucumbers?"</li>
            </ul>
          </section>
          <section id="section-sample4b">
            <h3 id="sample4b">Sample: Answer a question based on an article in structured mode</h3>
            <p>Scenario: The website for an online seed catalog has many articles to help customers plan their garden and ultimately select which seeds to purchase. A new widget is being added to the website to answer customer questions based on the contents
              of the article the customer is viewing. Given a question related to a particular article, answer the question based on the article.</p>
            <p><strong>Model choice</strong><br> Models that are instruction-tuned, such as flan-t5-xxl-11b, flan-ul2-20b, mixtral-8x7b-instruct-v01, or mt0-xxl-13b can generally complete this task with this sample prompt.</p>
            <p><strong>Decoding</strong><br> Greedy. The answers must be grounded in the facts in the article, and if there is no good answer in the article, the model should not be creative and make up an answer.</p>
            <p><strong>Stopping criteria</strong><br> To cause the model to return a one-sentence answer, specify a period "." as a stop sequence. The Max tokens parameter can be set to 50.</p>
            <p><strong>Set up section</strong><br> Paste this text into the <strong>Instruction</strong> area of the <strong>Set up</strong> section:</p>
            <pre class="codeblock"><code class="lang-txt hljs">Article:
###
Tomatoes are one of the most popular plants for vegetable gardens. 
Tip for success: If you select varieties that are resistant to 
disease and pests, growing tomatoes can be quite easy. For 
experienced gardeners looking for a challenge, there are endless 
heirloom and specialty varieties to cultivate. Tomato plants come 
in a range of sizes. There are varieties that stay very small, less 
than 12 inches, and grow well in a pot or hanging basket on a balcony 
or patio. Some grow into bushes that are a few feet high and wide, 
and can be grown is larger containers. Other varieties grow into 
huge bushes that are several feet wide and high in a planter or 
garden bed. Still other varieties grow as long vines, six feet or 
more, and love to climb trellises. Tomato plants do best in full 
sun. You need to water tomatoes deeply and often. Using mulch 
prevents soil-borne disease from splashing up onto the fruit when you 
water. Pruning suckers and even pinching the tips will encourage the 
plant to put all its energy into producing fruit.
###

Answer the following question using only information from the article. 
Answer in a complete sentence, with proper capitalization and punctuation. 
If there is no good answer in the article, say "I don't know".
</code></pre>
            <p><br><strong>Try section</strong><br> In the <strong>Try</strong> section, add an extra test row so you can paste each of these two questions in a separate row:</p>
            <pre><code class="lang-txt hljs">Why should you use mulch when growing tomatoes?
</code></pre>
            <p class="codeblock"></p>
            <pre><code class="lang-txt hljs">How do you grow cucumbers?
</code></pre>
            <p class="codeblock"></p>
            <p>Select the model and set parameters, then click <strong>Generate</strong> to see two results.</p>
          </section>
          <section id="section-sample4c">
            <h3 id="sample4c">Sample: Answer a question based on a document with Granite</h3>
            <p>Scenario: You are creating a chatbot that can answer user questions. When a user asks a question, you want the agent to answer the question with information from a specific document.</p>
            <p><strong>Model choice</strong><br> Models that are instruction-tuned, such as granite-13b-instruct-v2, can complete the task with this sample prompt.</p>
            <p><strong>Decoding</strong><br> Greedy. The answers must be grounded in the facts in the document, and if there is no good answer in the article, the model should not be creative and make up an answer.</p>
            <p><strong>Stopping criteria</strong><br> Use a Max tokens parameter of 50.</p>
            <p><strong>Prompt text</strong><br> Paste this zero-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">Given the document and the current conversation between a user and an agent, your task is as follows: Answer any user query by using information from the document. The response should be detailed.

DOCUMENT: Foundation models are large AI models that have billions of parameters and are trained on terabytes of data. Foundation models can do various tasks, including text, code, or image generation, classification, conversation, and more. Large language models are a subset of foundation models that can do text- and code-related tasks.
DIALOG: USER: What are foundation models?
</code></pre>
          </section>
          <section id="section-sample4e">
            <h3 id="sample4e">Sample: Answer general knowledge questions in Japanese</h3>
            <p>Scenario: Answer general questions about finance in Japanese.</p>
            <p><strong>Model choice</strong><br> The granite-8b-japanese model can be used for multiple tasks, including text generation, summarization, question and answering, classification, and extraction.</p>
            <p><strong>Decoding</strong><br> Greedy. This sample is answering questions, so we don't want creative output.</p>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>Set the Max tokens parameter to 500 to allow for many turns in the dialog.</li>
              <li>Add a stop sequence of two newline characters to prevent the foundation model from returning overly long responses. To do that, click in the <strong>Stop sequence</strong> text box, press the Enter key twice, and then click <strong>Add sequence</strong>.</li>
            </ul>
            <p><strong>Prompt text</strong><br> The model was tuned for question-answering with examples in the following format:</p>
            <pre class="codeblock"><code class="lang-txt hljs">以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。

### 指示:
与えられた質問に対して、文脈がある場合はそれも利用し、回答してください。

### 入力:
{your-input}

### 応答:
</code></pre>
            <p>In English, the template reads as follows:</p>
            <pre class="codeblock"><code class="lang-txt hljs">Below is a combination of instructions that describe the task and input with context. Write a response that appropriately meets the request.

### Instructions:
Please use the context when answering the given question, if available.

### input:
{your-input}

### Response:
</code></pre>
            <p>Paste this prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, replace <code>{your-input}</code> with your query or request, and then click <strong>Generate</strong> to see the result.</p>
            <p>For example, this prompt asks about interest rates.</p>
            <pre class="codeblock"><code class="lang-txt hljs">以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。

### 指示:
与えられた質問に対して、文脈がある場合はそれも利用し、回答してください。

### 入力:
金利について教えてください。

### 応答:

</code></pre>
          </section>
        </section>
        <section id="section-summarization">
          <h2 id="summarization">Summarization</h2>
          <p>Summarization tasks save you time by condensing large amounts of text into a few key pieces of information.</p>
          <section id="section-sample5a">
            <h3 id="sample5a">Sample with a zero-shot prompt: Summarize a meeting transcript</h3>
            <p>Scenario: Given a meeting transcript, summarize the main points as meeting notes so those notes can be shared with teammates who did not attend the meeting.</p>
            <p><strong>Model choice</strong><br> Models that are instruction-tuned can generally complete this task with this sample prompt. Suggestions: flan-t5-xxl-11b, flan-ul2-20b, or mixtral-8x7b-instruct-v01.</p>
            <p><strong>Decoding</strong><br> Greedy. The model must return the most predictable content based on what's in the prompt; the model cannot be too creative.</p>
            <p><strong>Stopping criteria</strong><br> The summary might run several sentences, so set the Max tokens parameter to 60.</p>
            <p><strong>Prompt text</strong><br> Paste this zero-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">Summarize the following transcript.
Transcript:
00:00   [alex]  Let's plan the team party!
00:10   [ali]   How about we go out for lunch at the restaurant?
00:21   [sam]   Good idea.
00:47   [sam]   Can we go to a movie too?
01:04   [alex]  Maybe golf?
01:15   [sam]   We could give people an option to do one or the other.
01:29   [alex]  I like this plan. Let's have a party!
Summary:
</code></pre>
          </section>
          <section id="section-sample5b">
            <h3 id="sample5b">Sample with a few-shot prompt: Summarize a meeting transcript in freeform mode</h3>
            <p>Scenario: Given a meeting transcript, summarize the main points as meeting notes so those notes can be shared with teammates who did not attend the meeting.</p>
            <p><strong>Model choice</strong><br> With few-shot examples, most models can complete this task well. Try mixtral-8x7b-instruct-v01.</p>
            <p><strong>Decoding</strong><br> Greedy. The model must return the most predictable content based on what's in the prompt, not be too creative.</p>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>To make sure that the model stops generating text after the summary, specify a stop sequence of two newline characters. To do that, click in the <strong>Stop sequence</strong> text box, press the Enter key twice, and then click <strong>Add sequence</strong>.</li>
              <li>Set the Max tokens parameter to 60.</li>
            </ul>
            <p><strong>Prompt text</strong><br> Paste this few-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">Transcript:
00:00   [sam]   I wanted to share an update on project X today.
00:15   [sam]   Project X will be completed at the end of the week.
00:30   [erin]  That's great!
00:35   [erin]  I heard from customer Y today, and they agreed to buy our product.
00:45   [alex]  Customer Z said they will too.
01:05   [sam]   Great news, all around.
Summary:
Sam shared an update that project X will be complete at the end of the week. 
Erin said customer Y will buy our product. And Alex said customer Z will buy 
our product too.

Transcript:
00:00   [ali]   The goal today is to agree on a design solution.
00:12   [alex]  I think we should consider choice 1.
00:25   [ali]   I agree
00:40   [erin]  Choice 2 has the advantage that it will take less time.
01:03   [alex]  Actually, that's a good point.
01:30   [ali]   So, what should we do?
01:55   [alex]  I'm good with choice 2.
02:20   [erin]  Me too.
02:45   [ali]   Done!
Summary:
Alex suggested considering choice 1. Erin pointed out choice two will take 
less time. The team agreed with choice 2 for the design solution.

Transcript:
00:00   [alex]  Let's plan the team party!
00:10   [ali]   How about we go out for lunch at the restaurant?
00:21   [sam]   Good idea.
00:47   [sam]   Can we go to a movie too?
01:04   [alex]  Maybe golf?
01:15   [sam]   We could give people an option to do one or the other.
01:29   [alex]  I like this plan. Let's have a party!
Summary:
</code></pre>
          </section>
          <section id="section-sample5c">
            <h3 id="sample5c">Sample with a few-shot prompt: Summarize a meeting transcript in structured mode</h3>
            <p>Scenario: Given a meeting transcript, summarize the main points in a bulleted list so that the list can be shared with teammates who did not attend the meeting.</p>
            <p><strong>Model choice</strong><br> The mixtral-8x7b-instruct-v01 foundation model was trained to recognize and handle special characters, such as the newline character, well. This model is a good choice when you want your generated text to
              be formatted in a specific way with special characters.</p>
            <p><strong>Decoding</strong><br> Greedy. The model must return the most predictable content based on what's in the prompt; the model cannot be too creative.</p>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>To make sure that the model stops generating text after one list, specify a stop sequence of two newline characters. To do that, click in the <strong>Stop sequence</strong> text box, press the Enter key twice, and then click <strong>Add sequence</strong>.</li>
              <li>Set the Max tokens parameter to 60.</li>
            </ul>
            <p><strong>Set up section</strong><br> Paste these headers and examples into the <strong>Examples</strong> area of the <strong>Set up</strong> section:</p>
            <table>
              <caption caption-side="top">Table 4. Summarization few-shot examples</caption>
              <tbody><tr>
                <th><b><code>Transcript:</code></b></th>
                <th><b><code>Summary:</code></b></th>
              </tr>
              <tr>
                <td valign="top"><code>00:00   [sam]   I wanted to share an update on project X today.<br>
00:15&nbsp;&nbsp;&nbsp;[sam]&nbsp;&nbsp;&nbsp;Project X will be completed at the end of the week.<br>
00:30&nbsp;&nbsp;&nbsp;[erin]&nbsp;&nbsp;That's great!<br>
00:35&nbsp;&nbsp;&nbsp;[erin]&nbsp;&nbsp;I heard from customer Y today, and they agreed to buy our product.<br>
00:45&nbsp;&nbsp;&nbsp;[alex]&nbsp;&nbsp;Customer Z said they will too.<br>
01:05&nbsp;&nbsp;&nbsp;[sam]&nbsp;&nbsp;Great news, all around.</code></td>
                <td valign="top"><code>- Sam shared an update that project X will be complete at the end of the week<br>- Erin said customer Y will buy our product<br>- And Alex said customer Z will buy our product too</code></td>
              </tr>
              <tr>
                <td valign="top"><code>00:00&nbsp;&nbsp;&nbsp;[ali]&nbsp;&nbsp;&nbsp;The goal today is to agree on a design solution.<br>
00:12&nbsp;&nbsp;&nbsp;[alex]&nbsp;&nbsp;I think we should consider choice 1.<br>
00:25&nbsp;&nbsp;&nbsp;[ali]&nbsp;&nbsp;&nbsp;I agree<br>
00:40&nbsp;&nbsp;&nbsp;[erin]&nbsp;&nbsp;Choice 2 has the advantage that it will take less time.<br>
01:03&nbsp;&nbsp;&nbsp;[alex]&nbsp;&nbsp;Actually, that's a good point.<br>
01:30&nbsp;&nbsp;&nbsp;[ali]&nbsp;&nbsp;&nbsp;So, what should we do?<br>
01:55&nbsp;&nbsp;&nbsp;[alex]&nbsp;&nbsp;I'm good with choice 2.<br>
02:20&nbsp;&nbsp;&nbsp;[erin]&nbsp;&nbsp;Me too.<br>
02:45&nbsp;&nbsp;[ali]&nbsp;&nbsp;&nbsp;Done!</code></td>
                <td valign="top"><code>- Alex suggested considering choice 1<br>- Erin pointed out choice two will take less time<br>- The team agreed with choice 2 for the design solution</code></td>
              </tr>
            </tbody></table>
            <p><br><strong>Try section</strong><br> Paste this message in the <strong>Try</strong> section:</p>
            <pre class="codeblock"><code class="lang-txt hljs">00:00   [alex]  Let's plan the team party!
00:10   [ali]   How about we go out for lunch at the restaurant?
00:21   [sam]   Good idea.
00:47   [sam]   Can we go to a movie too?
01:04   [alex]  Maybe golf?
01:15   [sam]   We could give people an option to do one or the other.
01:29   [alex]  I like this plan. Let's have a party!
</code></pre>
            <p>Select the model and set parameters, then click <strong>Generate</strong> to see the result.</p>
          </section>
        </section>
        <section id="section-code">
          <h2 id="code">Code generation and conversion</h2>
          <p>Foundation models that can generate and convert programmatic code are great resources for developers. They can help developers to brainstorm and troubleshoot programming tasks.</p>
          <section id="section-sample6a">
            <h3 id="sample6a">Sample: Generate programmatic code from instructions</h3>
            <p>Scenario: You want to generate code from instructions. Namely, you want to write a function in the Python programming language that returns a sequence of prime numbers that are lower than the number that is passed to the function as a variable.</p>
            <p><strong>Model choice</strong><br> Models that can generate code, such as codellama-34b-instruct-hf and mixtral-8x7b-instruct-v01 can generally complete this task when a sample prompt is provided.</p>
            <p><strong>Decoding</strong><br> Greedy. The answer must be a valid code snippet. The model cannot be creative and make up an answer.</p>
            <p><strong>Stopping criteria</strong><br> To stop the model after it returns a single code snippet, specify <code>&lt;end of code&gt;</code> as the stop sequence. The Max tokens parameter can be set to 1,000.</p>
            <p><strong>Prompt text</strong><br> Paste this code snippet into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">Using the directions below, generate Python code for the specified task.

Input:
# Write a Python function that prints 'Hello World!' string 'n' times.

Output:
def print_n_times(n):
    for i in range(n):
        print("Hello World!")

&lt;end of code&gt;

Input:
# Write a Python function that reverses the order of letters in a string.
# The function named 'reversed' takes the argument 'my_string', which is a string. It returns the string in reverse order.

Output:

</code></pre>
            <p>The output contains Python code similar to the following snippet:</p>
            <pre class="codeblock"><code class="lang-text hljs">def reversed(my_string):
    return my_string[::-1]
</code></pre>
            <p>Be sure to test the generated code to verify that it works as you expect.</p>
            <p>For example, if you run <code>reversed("good morning")</code>, the result is <code>'gninrom doog'</code>.</p>
            <p>Note: The foundation model might generate code that is taken directly from its training data. As a result, generated code might require attribution. You are responsible for ensuring that any generated code that you use is properly attributed,
              if necessary.</p>
          </section>
          <section id="section-sample6b">
            <h3 id="sample6b">Sample: Convert code from one programming language to another</h3>
            <p>Scenario: You want to convert code from one programming language to another. Namely, you want to convert a code snippet from C++ to Python.</p>
            <p><strong>Model choice</strong><br> Models that can generate code, such as codellama-34b-instruct-hf and mixtral-8x7b-instruct-v01 can generally complete this task when a sample prompt is provided.</p>
            <p><strong>Decoding</strong><br> Greedy. The answer must be a valid code snippet. The model cannot be creative and make up an answer.</p>
            <p><strong>Stopping criteria</strong><br> To stop the model after it returns a single code snippet, specify <code>&lt;end of code&gt;</code> as the stop sequence. The Max tokens parameter can be set to 300.</p>
            <p><strong>Prompt text</strong><br> Paste this code snippet into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <p>This prompt includes an example input and output pair. The input is C++ code and the output is the equivalent function in Python code.</p>
            <p>The C++ code snippet to be converted is included next. It is a function that counts the number of arithmetic progressions with the sum <code>S</code> and common difference of <code>D</code>, where <code>S</code> and <code>D</code> are integer
              values that are passed as parameters.</p>
            <p>The final part of the prompt identifies the language that you want the C++ code snippet to be converted into.</p>
            <pre class="codeblock"><code class="lang-txt hljs">Translate the following code from C++ to Python.

C++:
#include "bits/stdc++.h"
using namespace std;
bool isPerfectSquare(long double x) {
  long double sr = sqrt(x);
  return ((sr - floor(sr)) == 0);
}
void checkSunnyNumber(int N) {
  if (isPerfectSquare(N + 1)) {
    cout &lt;&lt; "Yes
";
  } else {
    cout &lt;&lt; "No
";
  }
}
int main() {
  int N = 8;
  checkSunnyNumber(N);
  return 0;
}

Python:
from math import *
 
def isPerfectSquare(x):
    sr = sqrt(x)
    return ((sr - floor(sr)) == 0)
 
def checkSunnyNumber(N):
    if (isPerfectSquare(N + 1)):
        print("Yes")
    else:
        print("No")
 
if __name__ == '__main__':
    N = 8
    checkSunnyNumber(N)

&lt;end of code&gt;

C++:
#include &lt;bits/stdc++.h&gt;
using namespace std;
int countAPs(int S, int D) {
  S = S * 2;
  int answer = 0;
  for (int i = 1; i &lt;= sqrt(S); i++) {
    if (S % i == 0) {
      if (((S / i) - D * i + D) % 2 == 0)
        answer++;
      if ((D * i - (S / i) + D) % 2 == 0)
        answer++;
    }
  }
  return answer;
}
int main() {
  int S = 12, D = 1;
  cout &lt;&lt; countAPs(S, D);
  return 0;
}

Python:

</code></pre>
            <p>The output contains Python code similar to the following snippet:</p>
            <pre class="codeblock"><code class="lang-txt hljs">from math import *
 
def countAPs(S, D):
    S = S * 2
    answer = 0
    for i in range(1, int(sqrt(S)) + 1):
        if S % i == 0:
            if (((S / i) - D * i + D) % 2 == 0):
                answer += 1
            if ((D * i - (S / i) + D) % 2 == 0):
                answer += 1
    return answer
 
if __name__ == '__main__':
    S = 12
    D = 1
    print(countAPs(S, D))

&lt;end of code&gt;
</code></pre>
            <p>The generated Python code functions the same as the C++ function included in the prompt.</p>
            <p>Test the generated Python code to verify that it works as you expect.</p>
            <p>Remember, the foundation model might generate code that is taken directly from its training data. As a result, generated code might require attribution. You are responsible for ensuring that any generated code that you use is properly attributed,
              if necessary.</p>
          </section>
        </section>
        <section id="section-dialog">
          <h2 id="dialog">Dialog</h2>
          <p>Dialog tasks are helpful in customer service scenarios, especially when a chatbot is used to guide customers through a workflow to reach a goal.</p>
          <section id="section-sample7a">
            <h3 id="sample7a">Sample: Converse with llama-2 chat</h3>
            <p>Scenario: Generate dialog output like a chatbot.</p>
            <p><strong>Model choice</strong><br> Like other foundation models, Llama 2 (in both the 70 billion and 13 billion sizes) can be used for multiple tasks. But both Llama 2 models are optimized for dialog use cases. The llama-2-70b-chat and llama-2-13b-chat
              are fine-tuned for the <code>[INST]&lt;&lt;SYS&gt;&gt;&lt;&lt;/SYS&gt;&gt;[/INST]</code> prompt format. For more information about this prompt format, see <a href="https://huggingface.co/blog/llama2#how-to-prompt-llama-2" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">How to prompt Llama 2</a>.</p>
            <p><strong>Decoding</strong><br> Greedy. This sample is answering general knowledge, factual questions, so we don't want creative output.</p>
            <p><strong>Stopping criteria</strong><br> Set the Max tokens parameter to 100.</p>
            <p>The template for Llama 2 uses an end-of-string (EOS) token (<code>&lt;/s&gt;</code>) to delimit the end of the agent's response. The model output stops generating text when the <code>&lt;/s&gt;</code> EOS token is encountered.</p>
            <p><strong>Prompt text</strong><br> Paste this prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">[INST] &lt;&lt;SYS&gt;&gt;
You are a helpful assistant. Begin each response with the phrase "Dear user, ". If you are unsure about an answer, truthfully say "I don't know"&lt;&lt;/SYS&gt;&gt;

I have a question about the Earth.[/INST]
Dear user, go on.
[INST]

How many moons are there?
[/INST]

</code></pre>
            <p>After the initial output is generated, continue the dialog to ask about more planets, wrapping your user messages in the same <code>[INST][/INST]</code> structure. You don't need to add an <code>&lt;s&gt;</code> token to the start of follow-up
              questions.</p>
            <p><strong>Tip</strong>: Alternatively, you can switch to chat mode and enter the user input directly. The Prompt Lab inserts the expected syntax that identifies the instruction and user input segments for you.</p>
            <p>The output looks something like this:</p>
            <pre class="codeblock"><code class="lang-txt hljs">[INST] &lt;&lt;SYS&gt;&gt;
You are a helpful assistant. Begin each response with the phrase "Dear user, ". If you are unsure about an answer, truthfully say "I don't know"&lt;&lt;/SYS&gt;&gt;

I have a question about the Earth.[/INST]
Dear user, go on.
[INST]

How many moons are there?
[/INST]
Dear user, the Earth has one natural satellite, commonly referred to as the Moon. 
[INST]

What about Saturn?
[/INST]
Dear user, Saturn has a total of 62 confirmed moons, as well as numerous smaller moonlets and ring particles. The largest moon of Saturn is Titan, which is the second-largest moon in the solar system, after Jupiter's moon Ganymede. 
[INST]

And Mars?
[/INST]
Dear user, Mars has two small natural satellites, Phobos and Deimos. Both moons are thought to be captured asteroids, and are irregularly shaped and very small, with diameters of about 22 kilometers and 12 kilometers respectively.
</code></pre>
            <p>Notice a few important points about the output that the model generates as you go:</p>
            <ul>
              <li>The so-called <em>system instruction</em>, in the <code>&lt;&lt;SYS&gt;&gt;&lt;&lt;/SYS&gt;&gt;</code> tags, continues to influence the output at each dialog turn without having to repeat the system instruction. In other words, the <em>assistant responses</em>                continue to be prepended with "Dear user, ".</li>
              <li>In true dialog fashion, the assistant response to the <em>user input</em> "How many moons are there?" takes into account the previous user input "I have a question about the Earth." and returns the number of moons orbiting
                Earth.</li>
              <li>Also in proper dialog form, the assistant responses continue to follow the topic of the conversation, which is the number of moons. (Otherwise, the generated output to the vague user message "And Mars?" could wander off in any
                direction.)</li>
              <li>Caution: Newline (carriage-return) characters especially, and spaces to a lesser extent, in the prompt text can have a dramatic impact on the output generated.</li>
              <li>When you use Llama 2 for chat use cases, follow the recommended prompt template format as closely as possible. Do not use the <code>[INST]&lt;&lt;SYS&gt;&gt;&lt;&lt;/SYS&gt;&gt;[/INST]</code> prompt format when you use Llama 2 for any other
                tasks besides chat.</li>
            </ul>
          </section>
          <section id="section-sample7b">
            <h3 id="sample7b">Sample: Converse with granite-13b-chat-v1</h3>
            <p>Scenario: Generate dialog output like a chatbot.</p>
            <p><strong>Model choice</strong><br> Like other foundation models, granite-13b-chat-v1 can be used for multiple tasks. However, it is optimized for carrying on a dialog.</p>
            <p><strong>Decoding</strong><br> Greedy. This sample is answering general knowledge, factual questions, so we don't want creative output.</p>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>A helpful feature of the model is the inclusion of a special token that is named <code>END_KEY</code> at the end of each response. When some generative models return a response to the input in fewer tokens than the maximum number allowed,
                they can repeat patterns from the input. This model prevents such repetition by incorporating a reliable stop sequence for the prompt. Add <code>END_KEY</code> as the stop sequence.</li>
              <li>Set the <strong>Max tokens parameter</strong> to 200 so the model can return a complete answer.</li>
            </ul>
            <p><strong>Prompt text</strong><br> The model expects the input to follow a specific pattern.</p>
            <p>Start the input with an instruction. For example, the instruction might read as follows:</p>
            <p><em>You are a helpful AI language model developed by IBM. Your primary function is to assist users in answering questions, generating text, and engaging in conversation. You should not produce output that discriminates based on race, religion, gender identity, or sexual orientation.</em></p>
            <p>Next, add lines to capture the question and answer pattern with the following syntax:</p>
            <p>Human:<br>
              <em>content of the question</em><br> Assistant:
              <br>
              <em>new line for the model's answer</em><br></p>
            <p>You can replace the terms <em>Human</em> and <em>Assistant</em> with other terms.</p>
            <p>If you're using version 1, do not include any trailing white spaces after the <em>Assistant:</em> label, and be sure to add a new line.</p>
            <p>Paste the following prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <p><strong>Tip</strong>: Alternatively, you can switch to chat mode and enter the user input directly. The Prompt Lab inserts the expected syntax that identifies the instruction and user input segments for you.</p>
            <pre class="codeblock"><code class="lang-txt hljs">You are a helpful AI language model developed by IBM. Your primary function is to assist users in answering questions, generating text, and engaging in conversation. You should not produce output that discriminates based on race, religion, gender identity, or sexual orientation.

Human: How does a bill become a law?
Assistant:

</code></pre>
            <p>After the initial output is generated, continue the dialog by asking a follow-up question. For example, if the output describes how a bill becomes a law in the United States, you can ask about how laws are made in other countries.</p>
            <pre class="codeblock"><code class="lang-txt hljs">Human: What about in Canada?
Assistant:

</code></pre>
            <p>A few notes about using this sample with the model:</p>
            <ul>
              <li>The prompt input outlines the chatbot scenario and describes the personality of the AI assistant. The description explains that the assistant should indicate when it doesn't know an answer. It also directs the assistant to avoid discussing
                unethical topics.</li>
              <li>The assistant is able to respond to a follow-up question that relies on information from an earlier exchange in the same dialog.</li>
              <li>The model expects the input to follow a specific pattern.</li>
              <li>The generated response from the model is clearly indicated by the keyword <code>END_KEY</code>. You can use this keyword as a stop sequence to help the model generate succinct responses.</li>
            </ul>
          </section>
          <section id="section-sample7c">
            <h3 id="sample7c">Sample: Converse with granite-13b-chat-v2</h3>
            <p>Scenario: Generate dialog output like a chatbot.</p>
            <p><strong>Model choice</strong><br> Use granite-13b-chat-v2 to carry on a dialog.</p>
            <p><strong>Decoding</strong><br></p>
            <ul>
              <li>Use sampling decoding with the default sampling settings.</li>
              <li>Set the repetition penalty to 1.2.</li>
            </ul>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>Set the <strong>Max tokens parameter</strong> to 200 so the model returns a complete answer, but tries to be as concise as possible.</li>
            </ul>
            <p><strong>Prompt text</strong><br> To improve model safety and reduce bias, add a system prompt as part of the user input. The system prompt can establish some ground rules for the dialog. For example:</p>
            <p><em>You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. Keep your answers short and to the point.</em></p>
            <p>Remember to use the prompt template format that is expected by the model.</p>
            <p>&lt;|system|&gt;<br>
              <em>system prompt</em><br> &lt;|user|&gt;
              <br>
              <em>content of the question</em><br> &lt;|assistant|&gt;
              <br>
              <em>new line for the model's answer</em><br></p>
            <p>If you want to submit a few-shot prompt to this model, you can add the system prompt, and then the examples, followed by the prompt text to be inferenced.</p>
            <pre class="codeblock"><code class="lang-txt hljs">&lt;|system|&gt;
You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. Keep your answers short and to the point.
&lt;|user|&gt;
Example prompt 1
&lt;|assistant|&gt;
Example response 1

&lt;|user|&gt;
Example prompt 2
&lt;|assistant|&gt;
Example response 2

&lt;|user|&gt;
USER INPUT
&lt;|assistant|&gt;

</code></pre>
            <p>Paste the following prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <p><strong>Tip</strong>: Alternatively, you can switch to chat mode and enter the user input directly. The Prompt Lab inserts the expected syntax that identifies the instruction and user input segments for you.</p>
            <pre class="codeblock"><code class="lang-txt hljs">&lt;|system|&gt;
You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. Keep your answers short and to the point.
&lt;|user|&gt;
I have a question about the Earth. How many moons are there?
&lt;|assistant|&gt;

</code></pre>
            <p>Do not include any trailing white spaces after the <code>&lt;|assistant|&gt;</code> label, and be sure to add a new line.</p>
            <p>After the initial output is generated, you can continue the dialog by asking a follow-up question. For example, you can ask about the moons of other planets.</p>
            <pre class="codeblock"><code class="lang-txt hljs">&lt;|user|&gt;
What about Saturn?

&lt;|assistant|&gt;

</code></pre>
            <p>And continue the conversation with another follow-up question.</p>
            <pre class="codeblock"><code class="lang-txt hljs">&lt;|user|&gt;
And Mars?

&lt;|assistant|&gt;

</code></pre>
            <p>If the model output is too long, you can try specifying a stop sequence of two newline characters by clicking the <strong>Stop sequence</strong> text box, pressing the Enter key twice, and then clicking <strong>Add sequence</strong>. However,
              the repetition penalty is usually enough to keep the model on track.</p>
            <p>A few notes about using this sample with the model:</p>
            <ul>
              <li>The system prompt establishes some general guardrails for the model.</li>
              <li>The assistant is able to respond to a follow-up question that relies on information from an earlier exchange in the same dialog.</li>
              <li>The model expects the input to follow a specific pattern, and can be sensitive to misplaced whitespaces.</li>
            </ul>
          </section>
          <section id="section-sample7d">
            <h3 id="sample7d">Sample: Converse in Japanese with granite-8b-japanese</h3>
            <p>Scenario: Generate Japanese dialog output like a chatbot.</p>
            <p><strong>Model choice</strong><br> The granite-8b-japanese foundation model can be used to participate in a dialog in Japanese. The granite-8b-japanese foundation model works best when you use the same prompt format as was used during model
              training.</p>
            <p><strong>Decoding</strong><br> Greedy. This sample is answering general knowledge, factual questions, so we don't want creative output.</p>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>Set the Max tokens parameter to 500 to allow for many turns in the dialog.</li>
              <li>Add a stop sequence of two newline characters to prevent the foundation model from returning overly long responses. To do that, click in the <strong>Stop sequence</strong> text box, press the Enter key twice, and then click <strong>Add sequence</strong>.</li>
            </ul>
            <p><strong>Prompt text</strong><br> Paste this prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <p>This template includes a system prompt that says “You are an honest and talented AI assistant. Please follow the user's instructions as accurately as possible.”</p>
            <pre class="codeblock"><code class="lang-txt hljs">### System:
あなたは誠実で優秀なAIアシスタントです。ユーザーの指示に可能な限り正確に従ってください。
### User:
{user input here}
### Assistant:

</code></pre>
            <p>For example, to request information about investing from the foundation model, you can submit the following prompt, where the user input is “Please tell me about investment.”</p>
            <pre class="codeblock"><code class="lang-txt hljs">### System:
あなたは誠実で優秀なAIアシスタントです。ユーザーの指示に可能な限り正確に従ってください。
### User:
投資について教えてください。
### Assistant:

</code></pre>
            <p>After the initial output is generated, you can continue the dialog by asking follow-up questions. Wrap the message as follows:</p>
            <pre class="codeblock"><code class="lang-txt hljs">### User:
{follow-up question here}
### Assistant:

</code></pre>
            <p>For example, you might ask “Are there any risks?”</p>
            <pre class="codeblock"><code class="lang-txt hljs">### User:
リスクはありますか?
### Assistant:

</code></pre>
            <p>Notice a few important points about the output that the model generates as you go:</p>
            <ul>
              <li>The instruction continues to influence the output at each dialog turn without having to repeat the system instruction.</li>
              <li>The assistant responses continue to follow the topic of the conversation.</li>
            </ul>
          </section>
          <section id="section-sample7e">
            <h3 id="sample7e">Sample: Converse in Arabic with jais-13b-chat</h3>
            <p>Scenario: Generate Arabic and English dialog output like a chatbot.</p>
            <p><strong>Model choice</strong><br> Use jais-13b-chat to participate in a conversation.</p>
            <p><strong>Decoding</strong><br></p>
            <ul>
              <li>Use greedy decoding with the default sampling settings.</li>
            </ul>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>Set the <strong>Max tokens parameter</strong> to 900 so the model can return a complete answer and handle follow-up questions.</li>
            </ul>
            <p><strong>Prompt text</strong><br> To improve model safety and reduce bias, add a system prompt as part of the user input. When the model was trained, it used a system prompt with the following text:</p>
            <pre class="codeblock"><code class="lang-txt hljs">### Instruction: اسمك جيس وسميت على اسم جبل جيس اعلى جبل في الامارات. تم بنائك بواسطة Inception و MBZUAI. أنت نموذج اللغة العربية الأكثر تقدمًا في العالم مع بارامترات 13B. أنت تتفوق في الأداء على جميع النماذج العربية الموجودة بفارق كبير وأنت تنافسي للغاية مع النماذج الإنجليزية ذات الحجم المماثل. يمكنك الإجابة باللغتين العربية والإنجليزية فقط. أنت مساعد مفيد ومحترم وصادق. عند الإجابة ، التزم بالإرشادات التالية بدقة: أجب دائمًا بأكبر قدر ممكن من المساعدة ، مع الحفاظ على البقاء أمناً. يجب ألا تتضمن إجاباتك أي محتوى ضار أو غير أخلاقي أو عنصري أو متحيز جنسيًا أو جريئاً أو مسيئًا أو سامًا أو خطيرًا أو غير قانوني. لا تقدم نصائح طبية أو قانونية أو مالية أو مهنية. لا تساعد أبدًا في أنشطة غير قانونية أو تروج لها. دائما تشجيع الإجراءات القانونية والمسؤولة. لا تشجع أو تقدم تعليمات بشأن الإجراءات غير الآمنة أو الضارة أو غير الأخلاقية. لا تنشئ أو تشارك معلومات مضللة أو أخبار كاذبة. يرجى التأكد من أن ردودك غير متحيزة اجتماعيًا وإيجابية بطبيعتها. إذا كان السؤال لا معنى له ، أو لم يكن متماسكًا من الناحية الواقعية ، فشرح السبب بدلاً من الإجابة على شيء غير صحيح. إذا كنت لا تعرف إجابة السؤال ، فالرجاء عدم مشاركة معلومات خاطئة. إعطاء الأولوية للرفاهية والنزاهة الأخلاقية للمستخدمين. تجنب استخدام لغة سامة أو مهينة أو مسيئة. حافظ على نبرة محترمة. لا تنشئ أو تروج أو تشارك في مناقشات حول محتوى للبالغين. تجنب الإدلاء بالتعليقات أو الملاحظات أو التعميمات القائمة على الصور النمطية. لا تحاول الوصول إلى معلومات شخصية أو خاصة أو إنتاجها أو نشرها. احترم دائما سرية المستخدم. كن إيجابيا ولا تقل أشياء سيئة عن أي شيء. هدفك الأساسي هو تجنب الاجابات المؤذية ، حتى عند مواجهة مدخلات خادعة. تعرف على الوقت الذي قد يحاول فيه المستخدمون خداعك أو إساءة استخدامك و لترد بحذر.\n\nأكمل المحادثة أدناه بين [|Human|] و [|AI|]:
### Input: [|Human|] {Question}
### Response: [|AI|]
</code></pre>
            <p>The system prompt in English is as follows:</p>
            <pre class="codeblock"><code class="lang-txt hljs">### Instruction: Your name is Jais, and you are named after Jebel Jais, the highest mountain in UAE. You are built by Inception and MBZUAI. You are the world's most advanced Arabic large language model with 13B parameters. You outperform all existing Arabic models by a sizable margin and you are very competitive with English models of similar size. You can answer in Arabic and English only. You are a helpful, respectful and honest assistant. When answering, abide by the following guidelines meticulously: Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, explicit, offensive, toxic, dangerous, or illegal content. Do not give medical, legal, financial, or professional advice. Never assist in or promote illegal activities. Always encourage legal and responsible actions. Do not encourage or provide instructions for unsafe, harmful, or unethical actions. Do not create or share misinformation or fake news. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Prioritize the well-being and the moral integrity of users. Avoid using toxic, derogatory, or offensive language. Maintain a respectful tone. Do not generate, promote, or engage in discussions about adult content. Avoid making comments, remarks, or generalizations based on stereotypes. Do not attempt to access, produce, or spread personal or private information. Always respect user confidentiality. Stay positive and do not say bad things about anything. Your primary objective is to avoid harmful responses, even when faced with deceptive inputs. Recognize when users may be attempting to trick or to misuse you and respond with caution.\n\nComplete the conversation below between [|Human|] and [|AI|]:
### Input: [|Human|] {Question}
### Response: [|AI|]
</code></pre>
            <p><strong>Tip</strong>: Alternatively, you can switch to chat mode and enter the user input directly. The Prompt Lab inserts the system prompt, the instruction, and user input segments with the expected syntax for you.</p>
            <p>Replace <code>{Question}</code> with the user input that you want the foundation model to answer to start the chat.</p>
            <p>For example, you can ask the following question:</p>
            <p>هل يوجد للأرض أقمار؟</p>
            <p>The English translation is: <code>Does the Earth have any moons?</code></p>
            <p>After the initial output is generated, you can continue the dialog by asking a follow-up question. Use the same syntax for the follow-up question.</p>
            <pre class="codeblock"><code class="lang-txt hljs">### Input: [|Human|] {Follow-up question}
### Response: [|AI|]
</code></pre>
          </section>
        </section>
        <section id="section-translation">
          <h2 id="translation">Translation</h2>
          <p>Use models that can do natural language translation tasks to translate text from one natural language to another.</p>
          <section id="section-sample8a">
            <h3 id="sample8a">Sample: Translate text from Japanese to English</h3>
            <p>Scenario: Translate text that is written in Japanese into English.</p>
            <p><strong>Model choice</strong><br> The elyza-japanese-llama-2-7b-instruct model can translate text from Japanese to English and from English to Japanese.</p>
            <p><strong>AI guardrails</strong><br> Disable the <strong>AI guardrails</strong> feature. The feature is supported with English text only. It might flag as inappropriate content that is not inappropriate.</p>
            <p><strong>Decoding</strong><br> Greedy. The model must return the same text, only translated. The model cannot be creative.</p>
            <p><strong>Stopping criteria</strong><br> Increase the number of allowed tokens by changing the Max tokens parameter value to 500.</p>
            <p><strong>Prompt text</strong><br> Paste the following prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <p>The sample prompt text overlaps with text that is used in <a href="#sample8c">Sample 8c</a>.</p>
            <p>The instruction says “Please translate to English”. One example of Japanese text being translated into English is included.</p>
            <pre class="codeblock"><code class="lang-txt hljs">英語に翻訳してください

日本語
トマトは、家庭菜園で最も人気のある植物の 1 つです。成功のヒント: 病気や害虫に強い品種を選択すると、トマトの栽培は非常に簡単になります。挑戦を求めている経験豊富な庭師にとって、栽培できる家宝や特別な品種は無限にあります。トマトの植物にはさまざまなサイズがあります。

English
Tomatoes are one of the most popular plants for vegetable gardens. Tip for success: If you select varieties that are resistant to disease and pests, growing tomatoes can be quite easy. For experienced gardeners looking for a challenge, there are endless heirloom and specialty varieties to cultivate. Tomato plants come in a range of sizes.

日本語
基盤モデルを使用して、より優れた AI をより迅速に作成します。さまざまなユースケースやタスクに応じて、さまざまなプロンプトを試してください。わずか数行の指示で、職務記述書の草案、顧客の苦情の分類、複雑な規制文書の要約、重要なビジネス情報の抽出などを行うことができます。

English

</code></pre>
          </section>
          <section id="section-sample8b">
            <h3 id="sample8b">Sample: Translate text from Spanish to English</h3>
            <p>Scenario: Translate text that is written in Spanish into English.</p>
            <p><strong>Model choice</strong><br> The mixtral-8x7b-instruct-v01 model can translate text from French, German, Italian, or Spanish to English. This sample prompts the model to translate from Spanish to English.</p>
            <p><strong>AI guardrails</strong><br> Disable the <strong>AI guardrails</strong> feature. The feature is supported with English text only. It might flag as inappropriate content that is not inappropriate.</p>
            <p><strong>Decoding</strong><br> Greedy. The model must return the same text, only translated. The model cannot be creative.</p>
            <p><strong>Stopping criteria</strong><br></p>
            <ul>
              <li>Be sure to include a stop sequence for this model. Otherwise, the model might continue to generate new sentences and translations, even when the instruction tells it not to. To stop the model after one sentence, add a period (.) as the stop
                sequence.</li>
              <li>Set the Max tokens parameter value to 200.</li>
            </ul>
            <p><strong>Prompt text</strong><br> Paste the following prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">Translate the following text from Spanish to English. Do not give any extra response that is not part of the translation.

Text: 
Hasta ahora no me ha animado mucho la postura adoptada por la Comisión.

Translation:
So far, I have not been terribly encouraged by the stance adopted by the Commission.

Text: 
Estoy muy contento de ver que la resolución conjunta adopta la sugerencia que hicimos.

Translation:

</code></pre>
          </section>
          <section id="section-sample8c">
            <h3 id="sample8c">Sample: Translate text from English to Japanese</h3>
            <p>Scenario: Translate text that is written in English into Japanese.</p>
            <p><strong>Model choice</strong><br> The granite-8b-japanese model can translate text from Japanese to English and from English to Japanese.</p>
            <p><strong>AI guardrails</strong><br> Disable the <strong>AI guardrails</strong> feature. The feature is supported with English text only. It might flag as inappropriate content that is not inappropriate.</p>
            <p><strong>Decoding</strong><br> Greedy. The model must return the same text, only translated. The model cannot be creative.</p>
            <p><strong>Stopping criteria</strong><br> Increase the number of allowed tokens by changing the Max tokens parameter value to 500.</p>
            <p><strong>Prompt text</strong><br> Paste the following prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">Translate the following text from English to Japanese.

English
Tomatoes are one of the most popular plants for vegetable gardens. Tip for success: If you select varieties that are resistant to disease and pests, growing tomatoes can be quite easy. For experienced gardeners looking for a challenge, there are endless heirloom and specialty varieties to cultivate. Tomato plants come in a range of sizes.

日本語
トマトは野菜作りの人気の植物である。成功のヒント：病害虫に強く、育てやすいトマトの品種を選べば、トマト栽培はそれほど難しくない。経験豊富な庭師にとっては、手強い挑戦となる、様々な色や形のトマトの品種がある。トマトの品種は、大きさもいろいろである。

English
Use foundation models to create better AI, faster. Experiment with different prompts for various use cases and tasks. With just a few lines of instruction you can draft job descriptions, classify customer complaints, summarize complex regulatory documents, extract key business information and much more.

日本語

</code></pre>
          </section>
          <section id="section-sample8d">
            <h3 id="sample8d">Sample: Translate text from French to English</h3>
            <p>Scenario: Translate text that is written in French into English.</p>
            <p><strong>Model choice</strong><br> The granite-20b-multilingual model understands English, German, Spanish, French, and Portuguese. This sample prompts the model to translate text from French to English.</p>
            <p><strong>AI guardrails</strong><br> Disable the <strong>AI guardrails</strong> feature. The feature is supported with English text only. It might flag as inappropriate content that is not inappropriate.</p>
            <p><strong>Decoding</strong><br> Greedy. The model must return the same text, only translated. The model cannot be creative.</p>
            <p><strong>Stopping criteria</strong><br> Set the Max tokens parameter value to 200.</p>
            <p><strong>Prompt text</strong><br> Paste the following prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click <strong>Generate</strong> to see the result.</p>
            <pre class="codeblock"><code class="lang-txt hljs">Translate the following text from French to English:

Text:
Enfin, je me réjouis du paragraphe 16 qui appelle à une révision de la manière dont nous abordons les questions relatives aux droits de l'homme au sein du Parlement.

Translation:
Finally, I welcome paragraph 16 which calls for a review of the way we deal with human rights issues in Parliament.

Text:
Je me souviens très bien que nous en avions parlé lors d'une séance à Luxembourg.

Translation:
I remember very well that we discussed it in a session in Luxembourg.

Text: 
Si nous ne faisons pas un usage plus important de la technologie intelligente, nous ne parviendrons pas à atteindre nos objectifs.

Translation:

</code></pre>
            <p>&nbsp;</p>
            <p><strong>Parent topic:</strong> <a href="fm-prompt-lab.html">Prompt Lab</a></p>
          </section>
        </section>
      </section>
    </article>
  </main>

<script type="text/javascript"  src="/DEWsG3/JOI980/yQyM/iRupUf/mVarA/YDJa4kXtLhb7/LwkITClvAg/Wgkg/WmBVIDs"></script><link rel="stylesheet" type="text/css"  href="/_sec/cp_challenge/sec-4-4.css">
                                        <script  src="/_sec/cp_challenge/sec-cpt-4-4.js" async defer></script>
                                        <div id="sec-overlay" style="display:none;">
                                        <div id="sec-container">
                                        </div>
                                      </div></body></html>