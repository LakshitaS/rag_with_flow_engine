<!DOCTYPE html><html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="DC.Publisher" content="IBM Corporation">
  <meta name="DC.rights" content="Â© Copyright IBM Corporation {{ copyright.years }}">
  <meta name="IBM.Country" content="ZZ">
  <meta name="DC.Date" content="{{ lastupdated }}">
  <meta name="keywords" content="{{ keywords }}">
  <meta name="subcollection" content="{{ subcollection }}">
  <meta name="description" content="Learn more about different tuning methods and how they work.">
  <meta name="content.type" content="topic">
  <meta name="tags" content="{{ services }}">
  <meta name="account.plan" content="{{ account-plan }}">
  <meta name="completion.time" content="{{ completion-time }}">
  <meta name="version" content="{{ version }}">
  <meta name="deployment.url" content="{{ deployment-url }}">
  <meta name="industry" content="{{ industry }}">
  <meta name="compliance" content="{{ compliance }}">
  <meta name="use.case" content="{{ use-case }}">
  <meta name="source.format" content="markdown">




  <!-- Licensed Materials - Property of IBM -->
  <!-- US Government Users Restricted Rights -->
  <!-- Use, duplication or disclosure restricted by -->
  <!-- GSA ADP Schedule Contract with IBM Corp. -->

  <title>Methods for tuning foundation models</title>
<link rel="canonical" href="https://www.ibm.com/docs/en/watsonx-as-a-service?topic=studio-methods-tuning"><meta name="viewport" content="width=device-width,initial-scale=1"></head>

<body>
  <main role="main">
    <article aria-labelledby="methods-for-tuning-foundation-models" role="article">
      <style>
        .midd::after {
          content: "\A\00A0\00A0";
          white-space: pre
        }
      </style>
      <section id="section-methods-for-tuning-foundation-models">
        <h1 id="methods-for-tuning-foundation-models">Methods for tuning foundation models</h1>
        <p>Learn more about different tuning methods and how they work.</p>
        <p>Models can be tuned in the following ways:</p>
        <ul>
          <li>
            <p><strong>Fine-tuning</strong>: Changes the parameters of the underlying foundation model to guide the model to generate output that is optimized for a task.</p>
            <div class="note note"><span class="notetitle">Note:</span> You currently cannot fine-tune models in Tuning Studio.</div>
          </li>
          <li>
            <p><strong>Prompt-tuning</strong>: Adjusts the content of the prompt that is passed to the model to guide the model to generate output that matches a pattern you specify. The underlying foundation model and its parameters are not edited. Only
              the prompt input is altered.</p>
            <p>When you prompt-tune a model, the underlying foundation model can be used to address different business needs without being retrained each time. As a result, you reduce computational needs and inference costs.</p>
          </li>
        </ul>
        <section id="section-how-prompt-tuning-works">
          <h2 id="how-prompt-tuning-works">How prompt-tuning works</h2>
          <p>Foundation models are sensitive to the input that you give them. Your input, or how you <em>prompt</em> the model, can introduce context that the model will use to tailor its generated output. Prompt engineering to find the <em>right</em> prompt
            often works well. However, it can be time-consuming, error-prone, and its effectiveness can be restricted by the context window length that is allowed by the underlying model.</p>
          <p>Prompt-tuning a model in the Tuning Studio applies machine learning to the task of prompt engineering. Instead of adding words to the input itself, prompt-tuning is a method for finding a sequence of values that, when added as a prefix to the
            input text, improve the model's ability to generate the output you want. This sequence of values is called a <em>prompt vector</em>.</p>
          <p>Normally, words in the prompt are vectorized by the model. Vectorization is the process of converting text to tokens, and then to numbers defined by the model's tokenizer to identify the tokens. Lastly, the token IDs are encoded, meaning they
            are converted into a vector representation, which is the input format that is expected by the embedding layer of the model. Prompt-tuning bypasses the model's text-vectorization process and instead crafts a prompt vector directly. This changeable
            prompt vector is concatenated to the vectorized input text and the two are passed as one input to the embedding layer of the model. Values from this crafted prompt vector affect the word embedding weights that are set by the model and influence
            the words that the model chooses to add to the output.</p>
          <p>To find the best values for the prompt vector, you run a tuning experiment. You demonstrate the type of output that you want for a corresponding input by providing the model with input and output example pairs in training data. With each training
            run of the experiment, the generated output is compared to the training data output. Based on what it learns from differences between the two, the experiment adjusts the values in the prompt vector. After many runs through the training data,
            the model finds the prompt vector that works best.</p>
          <p>You can choose to start the training process by providing text that is vectorized by the experiment. Or you can let the experiment use random values in the prompt vector. Either way, unless the initial values are exactly right, they will be
            changed repeatedly as part of the training process. Providing your own initialization text can help the experiment reach a good result more quickly.</p>
          <p>The result of the experiment is a tuned version of the underlying model. You submit input to the tuned model for inferencing and the model generates output that follows the tuned-for pattern.</p>
        </section>
        <section id="section-learn-more">
          <h2 id="learn-more">Learn more</h2>
          <ul>
            <li><a href="https://research.ibm.com/blog/what-is-ai-prompt-tuning" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">IBM Research blog post: What is prompt-tuning?</a></li>
            <li><a href="https://arxiv.org/abs/2104.08691" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">Research paper: The Power of Scale for Parameter-Efficient Prompt Tuning</a></li>
            <li><a href="fm-tuning-parameters.html">Tuning parameters</a></li>
          </ul>
          <p><strong>Parent topic:</strong> <a href="fm-tuning-studio.html">Tuning Studio</a></p>
        </section>
      </section>
    </article>
  </main>

<script type="text/javascript"  src="/DEWsG3/JOI980/yQyM/iRupUf/mVarA/YDJa4kXtLhb7/LwkITClvAg/Wgkg/WmBVIDs"></script><link rel="stylesheet" type="text/css"  href="/_sec/cp_challenge/sec-4-4.css">
                                        <script  src="/_sec/cp_challenge/sec-cpt-4-4.js" async defer></script>
                                        <div id="sec-overlay" style="display:none;">
                                        <div id="sec-container">
                                        </div>
                                      </div></body></html>