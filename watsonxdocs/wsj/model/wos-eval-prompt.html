<!DOCTYPE html><html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="DC.Publisher" content="IBM Corporation">
  <meta name="DC.rights" content="Â© Copyright IBM Corporation {{ copyright.years }}">
  <meta name="IBM.Country" content="ZZ">
  <meta name="DC.Date" content="{{ lastupdated }}">
  <meta name="keywords" content="{{ keywords }}">
  <meta name="subcollection" content="{{ subcollection }}">
  <meta name="description" content="You can evaluate prompt templates in projects with watsonx.governance to measure the performance of foundation model tasks and understand how your model generates responses.">
  <meta name="content.type" content="topic">
  <meta name="tags" content="{{ services }}">
  <meta name="account.plan" content="{{ account-plan }}">
  <meta name="completion.time" content="{{ completion-time }}">
  <meta name="version" content="{{ version }}">
  <meta name="deployment.url" content="{{ deployment-url }}">
  <meta name="industry" content="{{ industry }}">
  <meta name="compliance" content="{{ compliance }}">
  <meta name="use.case" content="{{ use-case }}">
  <meta name="source.format" content="markdown">




  <!-- Licensed Materials - Property of IBM -->
  <!-- US Government Users Restricted Rights -->
  <!-- Use, duplication or disclosure restricted by -->
  <!-- GSA ADP Schedule Contract with IBM Corp. -->

  <title>Evaluating prompt templates in projects</title>
<link rel="canonical" href="https://www.ibm.com/docs/en/watsonx-as-a-service?topic=models-evaluating-prompt-templates-in-projects"><meta name="viewport" content="width=device-width,initial-scale=1"></head>

<body>
  <main role="main">
    <article aria-labelledby="evaluating-prompt-templates-in-projects" role="article">
      <style>
        .midd::after {
          content: "\A\00A0\00A0";
          white-space: pre
        }
      </style>
      <section id="section-evaluating-prompt-templates-in-projects">
        <h1 id="evaluating-prompt-templates-in-projects">Evaluating prompt templates in projects</h1>
        <p>You can evaluate prompt templates in projects with watsonx.governance to measure the performance of foundation model tasks and understand how your model generates responses.</p>
        <p>With watsonx.governance, you can evaluate prompt templates in projects to measure how effectively your foundation models generate responses for the following task types:</p>
        <ul>
          <li><a href="../analyze-data/fm-prompt-samples.html#classification">Classification</a></li>
          <li><a href="../analyze-data/fm-prompt-samples.html#summarization">Summarization</a></li>
          <li><a href="../analyze-data/fm-prompt-samples.html#generation">Generation</a></li>
          <li><a href="../analyze-data/fm-prompt-samples.html#qa">Question answering</a></li>
          <li><a href="../analyze-data/fm-prompt-samples.html#extraction">Entity extraction</a></li>
        </ul>
        <section id="section-before-you-begin">
          <h2 id="before-you-begin">Before you begin</h2>
          <p>You must have access to a project to evaluate prompt templates. For more information, see <a href="wos-setup-wos.html">Setting up watsonx.governance</a>.</p>
          <p>To run evaluations, you must log in and <a href="../admin/personal-settings.html#account">switch</a> to a watsonx account that has watsonx.governance and watsonx.ai instances that are installed and open a project. You must be assigned the <strong>Admin</strong>            or <strong>Editor</strong> roles for the account to open projects.</p>
          <p>In your project, you must use the watsonx.ai <a href="../analyze-data/fm-prompt-lab.html">Prompt Lab</a> to create and save a prompt template. You must specify variables when you create prompt templates to enable evaluations. The <strong>Try</strong>            section in the Prompt Lab must contain at least one variable.</p>
          <p>Watch this video to see how to evaluate a prompt template in a project.</p>
          <div>
            <p style="font-size:smaller">This video provides a visual method to learn the concepts and tasks in this documentation.</p><iframe id="wm-wx-gov-evaluate-prompt" src="https://video.ibm.com/embed/channel/23952663/video/wx-gov-evaluate-prompt" lang="en-US" style="border: 0;" webkitallowfullscreen="" allowfullscreen="" frameborder="no" width="560" height="315" title="This video shows you how to evalluate a prompt template in a project." alt="This video shows you how to evalluate a prompt template in a project."></iframe></div>
          <p>The following sections describe how to evaluate prompt templates in projects and review your evaluation results.</p>
        </section>
        <section id="section-run-prompt-eval">
          <h2 id="run-prompt-eval">Running evaluations</h2>
          <p>To run prompt template evaluations, you can click <strong>Evaluate</strong> when you open a saved prompt template on the <strong>Assets</strong> tab in watsonx.governance to open the <strong>Evaluate prompt template</strong> wizard. You can
            run evaluations only if you are assigned the <strong>Admin</strong> or <strong>Editor</strong> roles for your project.</p>
          <p><img src="images/wos-run-eval-prompt.png" alt="Run prompt template evaluation"></p>
          <section id="section-select-dimensions">
            <h3 id="select-dimensions">Select dimensions</h3>
            <p>The <strong>Evaluate prompt template</strong> wizard displays the dimensions that are available to evaluate for the task type that is associated with your prompt. You can expand the dimensions to view the list of metrics that are used to evaluate
              the dimensions that you select.</p>
            <p><img src="images/wos-select-dimension-preprod-spaces.png" alt="Select dimensions to evaluate"></p>
            <p>Watsonx.governance automatically configures evaluations for each dimension with default settings. To <a href="wos-monitors-overview.html">configure evaluations</a> with different settings, you can select <strong>Advanced settings</strong>              to set minimum sample sizes and threshold values for each metric as shown in the following example:</p>
            <p><img src="images/wos-config-eval-settings.png" alt="Configure evaluations"></p>
          </section>
          <section id="section-select-test-data">
            <h3 id="select-test-data">Select test data</h3>
            <p>You must upload a CSV file that contains test data with reference columns and columns for each prompt variable. When the upload completes, you must also map <a href="../analyze-data/fm-prompt-variables.html#creating-prompt-variables">prompt variables</a>              to the associated columns from your test data.</p>
            <p><img src="images/wos-select-test-data.png" alt="Select test data to upload"></p>
          </section>
          <section id="section-review-prompt-eval-select">
            <h3 id="review-prompt-eval-select">Review and evaluate</h3>
            <p>Before you run your prompt template evaluation, you can review the selections for the prompt task type, the uploaded test data, and the type of evaluation that runs.</p>
            <p><img src="images/wos-review-prompt-eval-select.png" alt="Review and evaluate prompt template evaluation settings"></p>
          </section>
        </section>
        <section id="section-review-eval-results">
          <h2 id="review-eval-results">Reviewing evaluation results</h2>
          <p>When your evaluation completes, you can review a summary of your evaluation results on the <strong>Evaluate</strong> tab in watsonx.governance to gain insights about your model performance. The summary provides an overview of metric scores and
            violations of default score thresholds for your prompt template evaluations.</p>
          <p>If you are assigned the <strong>Viewer</strong> role for your project, you can select <strong>Evaluate</strong> from the asset list on the <strong>Assets</strong> tab to view evaluation results.</p>
          <p><img src="images/wos-run-eval-asset.png" alt="Run prompt template evaluation from asset list"></p>
          <p>To analyze results, you can click the arrow <img src="images/wos-nav-arrow.png" alt="navigation arrow"> next to your prompt template evaluation to view data visualizations of your results over time. You can also analyze results from the model
            health evaluation that is run by default during prompt template evaluations to understand how efficiently your model processes your data.</p>
          <p>The <strong>Actions</strong> menu also provides the following options to help you analyze your results:</p>
          <ul>
            <li><strong>Evaluate now</strong>: Run evaluation with a different test data set</li>
            <li><strong>All evaluations</strong>: Display a history of your evaluations to understand how your results change over time.</li>
            <li><strong>Configure monitors</strong>: Configure evaluation thresholds and sample sizes.</li>
            <li><strong>View model information</strong>: View details about your model to understand how your deployment environment is set up.</li>
          </ul>
          <p><img src="images/wos-analyze-prompt-eval-results.png" alt="Analyze prompt template evaluation results"></p>
          <p>If you <a href="../analyze-data/xgov-track-prompt-temp.html">track prompt templates</a>, you can review evaluation results to gain insights about your model performance throughout the AI lifecycle.</p>
          <p><strong>Parent topic:</strong> <a href="getting-started.html">Evaluating AI models with Watson OpenScale</a>.</p>
        </section>
      </section>
    </article>
  </main>

<script type="text/javascript"  src="/DEWsG3/JOI980/yQyM/iRupUf/mVarA/YDJa4kXtLhb7/LwkITClvAg/Wgkg/WmBVIDs"></script><link rel="stylesheet" type="text/css"  href="/_sec/cp_challenge/sec-4-4.css">
                                        <script  src="/_sec/cp_challenge/sec-cpt-4-4.js" async defer></script>
                                        <div id="sec-overlay" style="display:none;">
                                        <div id="sec-container">
                                        </div>
                                      </div></body></html>