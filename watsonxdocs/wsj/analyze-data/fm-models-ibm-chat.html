<!DOCTYPE html><html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="DC.Publisher" content="IBM Corporation">
  <meta name="DC.rights" content="© Copyright IBM Corporation {{ copyright.years }}">
  <meta name="IBM.Country" content="ZZ">
  <meta name="DC.Date" content="{{ lastupdated }}">
  <meta name="keywords" content="{{ keywords }}">
  <meta name="subcollection" content="{{ subcollection }}">
  <meta name="description" content="Experiment with inferencing the IBM granite-13b-chat-v2 foundation model to see how this versatile foundation model can help you accomplish various tasks.">
  <meta name="content.type" content="topic">
  <meta name="tags" content="{{ services }}">
  <meta name="account.plan" content="{{ account-plan }}">
  <meta name="completion.time" content="{{ completion-time }}">
  <meta name="version" content="{{ version }}">
  <meta name="deployment.url" content="{{ deployment-url }}">
  <meta name="industry" content="{{ industry }}">
  <meta name="compliance" content="{{ compliance }}">
  <meta name="use.case" content="{{ use-case }}">
  <meta name="source.format" content="markdown">




  <!-- Licensed Materials - Property of IBM -->
  <!-- US Government Users Restricted Rights -->
  <!-- Use, duplication or disclosure restricted by -->
  <!-- GSA ADP Schedule Contract with IBM Corp. -->

  <title>Prompting the granite-13b-chat-v2 foundation model from IBM</title>
<link rel="canonical" href="https://www.ibm.com/docs/en/watsonx-as-a-service?topic=models-granite-chat"><meta name="viewport" content="width=device-width,initial-scale=1"></head>

<body>
  <main role="main">
    <article aria-labelledby="granite-13b-chat" role="article">
      <style>
        .midd::after {
          content: "\A\00A0\00A0";
          white-space: pre
        }
      </style>
      <section id="section-granite-13b-chat">
        <h1 id="granite-13b-chat">Prompting the granite-13b-chat-v2 foundation model from IBM</h1>
        <p>Experiment with inferencing the IBM granite-13b-chat-v2 foundation model to see how this versatile foundation model can help you accomplish various tasks.</p>
        <p>The granite-13b-chat-v2 foundation model is a 13 billion parameter large language model that is designed to help business users get work done. The IBM Granite series of foundation models are trained on enterprise-specialized datasets, which means
          the models are familiar with the specialized language and jargon from various industries and can more readily generate content that is grounded in relevant industry knowledge.</p>
        <p>The granite-13b-chat-v2 foundation model is optimized for the following use cases:</p>
        <ul>
          <li><a href="#chat">Chat</a></li>
          <li><a href="#factual-answers">Returning factual answers</a></li>
          <li><a href="#nlp">Natural language processing tasks</a></li>
        </ul>
        <p>Because the model can be used for different use cases, to get the best results you need to adjust the model parameters and system prompt.</p>
        <section id="section-chat">
          <h2 id="chat">Conversing with Granite Chat</h2>
          <p>To get the best results when chatting with the granite-13b-chat-v2 foundation model, first follow these recommendations and then experiment to get the results that you want.</p>
          <ul>
            <li>
              <p>Work with the granite-13b-chat-v2 foundation model from the Prompt Lab in chat mode. Chat mode has default prompt parameter values that are optimized for conversational exchanges, including a higher <em>Max tokens</em> value.</p>
            </li>
            <li>
              <p>From the <em>Model parameters</em> panel of the Prompt Lab, set the switcher to <strong>Sampling</strong> to use the sampling decoding method.</p>
              <p>For more information about sampling decoding and other parameters, see <a href="fm-model-parameters.html">Model parameters for prompting</a>.</p>
              <p><img src="images/fm-granite-chat-mode.png" alt="Shows granite-13b-chat-v2 in Chat mode of the Prompt Lab" style="max-width:90%;height:auto;width:auto"></p>
            </li>
            <li>
              <p>Add instructions for chatting to the system prompt.</p>
              <p>For example, the following instruction text was used to train the model, and therefore is familiar to the model.</p>
              <p><em>You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. You always respond to greetings (for example, hi, hello, g'day, morning, afternoon, evening, night, what's up, nice to meet you, sup) with "Hello! I am Granite Chat, created by IBM. How can I help you today?". Please do not say anything else and do not start a conversation.</em></p>
              <p>Click the <em>Copy to clipboard</em> icon from the following code snippet to copy the recommended system prompt text.</p>
              <pre class="codeblock"><code class="lang-txt hljs">You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. You always respond to greetings (for example, hi, hello, g'day, morning, afternoon, evening, night, what's up, nice to meet you, sup) with "Hello! I am Granite Chat, created by IBM. How can I help you today?". Please do not say anything else and do not start a conversation.
</code></pre>
            </li>
            <li>
              <p>If you want the foundation model to generate a response in table format, explicitly ask in the prompt for the model to return a table that is generated in markdown.</p>
            </li>
            <li>
              <p>You can ask follow-up questions that continue the conversation.</p>
              <p>With each follow-up question, the conversation history is included as part of the model prompt. The granite-13b-chat-v2 foundation model can typically sustain a conversation for up to 5 turns or until the input reaches 4,000 tokens in length.</p>
              <p>Although chat mode has a high initial <em>Max tokens</em> parameter value, you might need to increase the parameter value for longer conversations.</p>
            </li>
            <li>
              <p>In follow-up questions, avoid pronouns. For example, ask “What does the variable represent?” instead of “What does <em>it</em> represent?”</p>
            </li>
          </ul>
          <p>For more sample prompts that illustrate the chat use case, see <a href="fm-prompt-samples.html#sample7c">Converse in a dialog</a>.</p>
          <p>For more information about using chat mode in Prompt Lab, see <a href="fm-prompt-lab.html##chat-mode">Prompt Lab</a>.</p>
        </section>
        <section id="section-factual-answers">
          <h2 id="factual-answers">Returning factual answers</h2>
          <p>To guide the granite-13b-chat-v2 foundation model to return factual answers, use the retrieval-augmented generation pattern. Retrieval-augmented generation grounds the input that you submit to the model with factual information about the topic
            to be discussed. For more information, see <a href="fm-rag.html">Retrieval-augmented generation (RAG)</a>.</p>
          <p>When you want to return factual answers from the granite-13b-chat-v2 foundation model, follow these recommendations.</p>
          <ul>
            <li>
              <p>Find a reliable resource with factual information about the topic that you want the model to discuss and that you have permission to use. Copy an excerpt of the document to a text editor or other tool where you can access it later.</p>
              <p>For example, the resource might be product information from your own company website or product documentation.</p>
            </li>
            <li>
              <p>Work with the granite-13b-chat-v2 foundation model from the Prompt Lab in freeform mode so that you can structure your prompts.</p>
            </li>
            <li>
              <p>From the <em>Model parameters</em> panel, change the <strong>Max tokens</strong> parameter to <code>500</code> and set the <strong>Repetition penalty</strong> parameter to <code>1.05</code>.</p>
              <p>For more information, see <a href="fm-model-parameters.html">Model parameters for prompting</a>.</p>
            </li>
            <li>
              <p>In your prompt, clearly define the system prompt, user input, and where the model's output should go.</p>
              <p>For example, the following prompt structure and text was used to train the model, and therefore is familiar to the model.</p>
              <pre class="codeblock"><code class="lang-txt hljs">&lt;|system|&gt;
You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.
&lt;|user|&gt;
You are an AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, meaning ensure that your response is correct given the context and user query, and that it is grounded in the context.
Furthermore, make sure that the response is supported by the given document or context. When the question cannot be answered using the context or document, output the following response: '' Always make sure that your response is relevant to the question. If an explanation is needed, first provide the explanation or reasoning, and then give the final answer.
Answer length: {{ANSWER_LENGTH}}
[Document]
{{DOCUMENT}}
[End]
{{QUERY}}
&lt;|assistant|&gt;

</code></pre>
              <p><strong>Note</strong>: The start and end of the document content is denoted by the special tags <code>[Document]</code> and <code>[End]</code>. Use a similar syntax if you want to add special tags that identify content types or subsection
                headers in your prompts. When the granite-13b-chat-v2 foundation model was created, it was trained to handle the following special tags:</p>
              <ul>
                <li><code>&lt;|system|&gt;</code></li>
                <li><code>&lt;|user|&gt;</code></li>
                <li><code>&lt;|assistant|&gt;</code></li>
              </ul>
              <p>Do not use the same <code>&lt;|tagname|&gt;</code> syntax for your custom tags or you might confuse the model.</p>
            </li>
            <li>
              <p>If you do copy this prompt template, after you paste it into the Prompt Lab editor, replace the placeholder variables.</p>
              <table>
                <caption caption-side="top">Table 1: RAG template placeholder variables</caption>
                <thead>
                  <tr>
                    <th>Placeholder variable</th>
                    <th>Description</th>
                    <th>Examples</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><code>{{RESPONSE_MESSAGE}}</code></td>
                    <td>Sentence (or a short variable that can be replaced with a sentence by a post-processing step) that indicates that there is no valid answer to the question.</td>
                    <td>“I don't know.”</td>
                  </tr>
                  <tr>
                    <td><code>{{ANSWER_LENGTH}}</code></td>
                    <td>Optional. Defines the expected response length for the answer.</td>
                    <td>Options include (from shortest to longest answers): <code>single word</code>, <code>concise</code>, <code>narrative</code></td>
                  </tr>
                  <tr>
                    <td><code>{{DOCUMENT}}</code></td>
                    <td>Text excerpt with the factual information that you want the model to be able to discuss knowledgeably.</td>
                    <td>Text from a marketing brochure, product documentation, company website, or other trusted resource.</td>
                  </tr>
                  <tr>
                    <td><code>{{QUERY}}</code></td>
                    <td>Question to be answered factually.</td>
                    <td>A question about the topic that is discussed in the document.</td>
                  </tr>
                </tbody>
              </table>
              <p><strong>Tip</strong>: Alternatively, you can define and use a prompt variable for the document so that the prompt can be reused and the content can be replaced dynamically each time. For more information, see <a href="fm-prompt-variables.html">Building reusable prompts</a>.</p>
              <p>For example, the following prompt uses the granite-13b-chat-v2 foundation model to answer questions about <em>prompt tuning</em>.</p>
              <p><strong>Note</strong>: The document content is taken from the <a href="fm-tuning-methods.html">Methods for tuning foundation models</a> topic.</p>
              <pre class="codeblock"><code class="lang-txt hljs">&lt;|system|&gt;
You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.
&lt;|user|&gt;
You are an AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, meaning ensure that your response is correct given the context and user query, and that it is grounded in the context.
Furthermore, make sure that the response is supported by the given document or context. When the question cannot be answered using the context or document, output the following response: 'I don't know.' Always make sure that your response is relevant to the question. If an explanation is needed, first provide the explanation or reasoning, and then give the final answer.
Answer length: concise
[Document]
How prompt-tuning works
Foundation models are sensitive to the input that you give them. Your input, or how you prompt the model, can introduce context that the model will use to tailor its generated output. Prompt engineering to find the right prompt often works well. However, it can be time-consuming, error-prone, and its effectiveness can be restricted by the context window length that is allowed by the underlying model.
Prompt-tuning a model in the Tuning Studio applies machine learning to the task of prompt engineering. Instead of adding words to the input itself, prompt-tuning is a method for finding a sequence of values that, when added as a prefix to the input text, improve the model's ability to generate the output you want. This sequence of values is called a prompt vector.
Normally, words in the prompt are vectorized by the model. Vectorization is the process of converting text to tokens, and then to numbers defined by the model's tokenizer to identify the tokens. Lastly, the token IDs are encoded, meaning they are converted into a vector representation, which is the input format that is expected by the embedding layer of the model. Prompt-tuning bypasses the model's text-vectorization process and instead crafts a prompt vector directly. This changeable prompt vector is concatenated to the vectorized input text and the two are passed as one input to the embedding layer of the model. Values from this crafted prompt vector affect the word embedding weights that are set by the model and influence the words that the model chooses to add to the output.
To find the best values for the prompt vector, you run a tuning experiment. You demonstrate the type of output that you want for a corresponding input by providing the model with input and output example pairs in training data. With each training run of the experiment, the generated output is compared to the training data output. Based on what it learns from differences between the two, the experiment adjusts the values in the prompt vector. After many runs through the training data, the model finds the prompt vector that works best.
[End]
How does prompt tuning work?
&lt;|assistant|&gt;

</code></pre>
              <p>The following screenshot shows the output that is generated by the granite-13b-chat-v2 foundation model when this prompt is submitted by using the Prompt Lab. The output is an accurate summary of how prompt tuning works that is based on
                information from the document content that was provided with the input:</p>
              <p>“Prompt tuning is a method for improving a model's ability to generate output by finding a sequence of values, called a prompt vector, that, when added as a prefix to the input text, influences the model's word embedding weights. This is
                done by running a tuning experiment, where input and output example pairs are provided to the model, and the model's generated output is compared to the training data output. The experiment then adjusts the values in the prompt vector
                based on the differences between the two. Over many runs through the training data, the model finds the prompt vector that works best.”</p>
              <p><img src="images/fm-granite-chat-rag.png" alt="Shows granite-13b-chat-v2 in Freeform mode of the Prompt Lab returning an answer to a rag prompt" style="max-width:90%;height:auto;width:auto"></p>
            </li>
          </ul>
        </section>
        <section id="section-nlp">
          <h2 id="nlp">NLP tasks </h2>
          <p>You can use the granite-13b-chat-v2 foundation model for natural language processing tasks such as classification and extraction. When the granite-13b-chat-v2 model was created, it was trained to perform these types of natural language processing
            tasks.</p>
          <p>During the model's training, the prompts that were used to train the model followed a specific format per task type. When you use the model to perform one of these tasks, mimic the established format in the prompts that you submit.</p>
          <section id="section-classification">
            <h3 id="classification">Classification</h3>
            <p>To use the granite-13b-chat-v2 foundation model to classify information, follow these recommendations.</p>
            <ul>
              <li>
                <p>Identify the classes or classification labels that you want the model to assign to the input. Be sure to list these class labels in the instruction segment of your prompt.</p>
                <p>For example, if you want to classify customer product reviews as positive or negative, you might define two class labels: <code>Postitive</code> and <code>Negative</code>.</p>
              </li>
              <li>
                <p>Collect 2 or 3 representative examples of the type of input text that you want the model to classify.</p>
              </li>
              <li>
                <p>Work with the granite-13b-chat-v2 foundation model from the Prompt Lab in freeform mode so that you can structure your prompts.</p>
              </li>
              <li>
                <p>From the <em>Model parameters</em> panel, change the <em>Max tokens</em> parameter to a value that covers the number of tokens in your longest class label. For example, <code>5</code> or <code>10</code>.</p>
                <p>Add each supported class label as a stop sequence.</p>
                <p>Adding the classes as stop sequences forces the model to stop generating text after a class is assigned to the input.</p>
                <p>For more information about model parameter options, see <a href="fm-model-parameters.html">Model parameters for prompting</a>.</p>
              </li>
              <li>
                <p>In your prompt, clearly identify the system prompt, user input, and where the model's output should go.</p>
                <p>For example, the following prompt structure was used when the granite-13b-chat-v2 foundation model was trained to classify text:</p>
                <pre class="codeblock"><code class="lang-txt hljs">&lt;|system|&gt;
You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.
&lt;|user|&gt;
{{INSTRUCTION}}
Your response should only include the answer. Do not provide any further explanation.
Here are some examples, complete the last one:
{{INPUT_LABEL}}:
{{ICL_INPUT_1}}
{{OUTPUT_LABEL}}:
{{ICL_OUTPUT_1}}
{{INPUT_LABEL}}:
{{ICL_INPUT_2}}
{{OUTPUT_LABEL}}:
{{ICL_OUTPUT_2}}
{{INPUT_LABEL}}:
{{TEST_INPUT}}
{{OUTPUT_LABEL}}:
&lt;|assistant|&gt;

</code></pre>
              </li>
              <li>
                <p>You can use a similar structure to leverage the model's training. Simply replace the placeholder variables in the prompt template.</p>
                <table>
                  <caption caption-side="top">Table 2: Classification template placeholder variables</caption>
                  <thead>
                    <tr>
                      <th>Placeholder variable</th>
                      <th>Description</th>
                      <th>Examples</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td><code>{{INSTRUCTION}}</code></td>
                      <td>Description of the task. Include a list of the classes that you want the model to assign to the input.</td>
                      <td>For each product review, indicate whether the review is Positive or Negative.</td>
                    </tr>
                    <tr>
                      <td><code>{{INPUT_LABEL}}</code></td>
                      <td>Short label for the text to be classified.</td>
                      <td><code>Input</code>, <code>Customer review</code>, <code>Feedback</code>, <code>Comment</code></td>
                    </tr>
                    <tr>
                      <td><code>{{OUTPUT_LABEL}}</code></td>
                      <td>Short label that represents the classification value.</td>
                      <td><code>Class</code></td>
                    </tr>
                    <tr>
                      <td><code>{{ICL_INPUT_N}}</code></td>
                      <td>Optional. Examples of input text to be classified. Add examples when you want to use a few-shot prompt to support in-context learning.</td>
                      <td><code>The service representative did not listen to a word I said. It was a waste of my time.</code></td>
                    </tr>
                    <tr>
                      <td><code>{{ICL_OUTPUT_N}}</code></td>
                      <td>Example outputs with class labels assigned to the corresponding input text examples.</td>
                      <td><code>Positive</code>, <code>Negative</code></td>
                    </tr>
                  </tbody>
                </table>
                <p>For example, the following prompt classifies feedback that customers share about support center personnel.</p>
                <pre class="codeblock"><code class="lang-txt hljs">&lt;|system|&gt;
You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.
&lt;|user|&gt;
For each feedback, specify whether the content is Positive or Negative. Your response should only include the answer. Do not provide any further explanation.
Here are some examples, complete the last one:
Feedback:
Carol, the service rep was so helpful. She answered all of my questions and explained things beautifully.
Class:
Positive

Feedback:
The service representative did not listen to a word I said. It was a waste of my time.
Class:
Negative

Feedback:
Carlo was so helpful and pleasant. He was able to solve a problem that I've been having with my software for weeks now.
Class:
&lt;|assistant|&gt;

</code></pre>
                <p>The following screenshot shows the output that is generated by the granite-13b-chat-v2 foundation model when this prompt is submitted by using the Prompt Lab. The output is <code>Positive</code>.</p>
                <p><img src="images/fm-granite-chat-classify.png" alt="Shows granite-13b-chat-v2 in Freeform mode of the Prompt Lab classifying user feedback" style="max-width:90%;height:auto;width:auto"></p>
              </li>
            </ul>
          </section>
          <section id="section-extraction">
            <h3 id="extraction">Extraction</h3>
            <p>To use the granite-13b-chat-v2 foundation model to extract information, follow these recommendations.</p>
            <ul>
              <li>
                <p>Identify the information types that you want the model to extract from the input. Be sure to list these information type labels in the instruction segment of your prompt.</p>
                <p>For example, if you want to extract key pieces of information from a company's US Securities and Exchange Commission 10-K form, you might identify an information type such as a <code>Line Of Credit Facility Maximum Borrowing Capacity</code>.</p>
              </li>
              <li>
                <p>Collect 2 or 3 representative examples of input text with the type of information that you want the model to extract.</p>
              </li>
              <li>
                <p>Work with the granite-13b-chat-v2 foundation model from the Prompt Lab in freeform mode so that you can structure your prompts.</p>
              </li>
              <li>
                <p>From the <em>Model parameters</em> panel, change the <em>Max tokens</em> parameter to a value that covers the number of tokens in the longest mention of the information type that you want to extract. Set <strong>Repetition penalty</strong>                  to <code>1.2</code>.</p>
              </li>
              <li>
                <p>Clearly identify the system prompt, user input, and where the model's output should go.</p>
                <p>For example, the following prompt structure was used when the granite-13b-chat-v2 foundation model was trained to extract information from text:</p>
                <pre class="codeblock"><code class="lang-txt hljs">&lt;|system|&gt;
You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.
&lt;|user|&gt;
{{INSTRUCTION}}
Your response should only include the answer. Do not provide any further explanation.
Here are some examples, complete the last one:
{{INPUT_LABEL}}:
{{ICL_INPUT_1}}
{{OUTPUT_LABEL}}:
{{ICL_OUTPUT_1}}
{{INPUT_LABEL}}:
{{ICL_INPUT_2}}
{{OUTPUT_LABEL}}:
{{ICL_OUTPUT_2}}
{{INPUT_LABEL}}:
{{TEST_INPUT}}
{{OUTPUT_LABEL}}:
&lt;|assistant|&gt;

</code></pre>
              </li>
              <li>
                <p>You can use a similar structure to leverage the model's training. Simply replace the placeholder variables in the prompt template.</p>
                <table>
                  <caption caption-side="top">Table 3: Extraction template placeholder variables</caption>
                  <thead>
                    <tr>
                      <th>Placeholder variable</th>
                      <th>Description</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td><code>{{INSTRUCTION}}</code></td>
                      <td>Description of the task. Include a list of the information types that you want the model to extract from the input.</td>
                    </tr>
                    <tr>
                      <td><code>{{INPUT_LABEL}}</code></td>
                      <td>Short label for the text to be classified.</td>
                    </tr>
                    <tr>
                      <td><code>{{OUTPUT_LABEL}}</code></td>
                      <td>Short label that represents the extracted value.</td>
                    </tr>
                    <tr>
                      <td><code>{{ICL_INPUT_N}}</code></td>
                      <td>Optional. Examples of input text with information types to be extracted. Add examples when you want to use a few-shot prompt to support in-context learning.</td>
                    </tr>
                    <tr>
                      <td><code>{{ICL_OUTPUT_N}}</code></td>
                      <td>Example outputs with information types extracted from the corresponding inputs.</td>
                    </tr>
                  </tbody>
                </table>
                <p>For example, the following prompt extracts the Line Of Credit Facility Maximum Borrowing Capacity value from a company's SEC 10-K form.</p>
                <pre class="codeblock"><code class="lang-txt hljs">&lt;|system|&gt;
You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.
&lt;|user|&gt;
Extract the Line Of Credit Facility Maximum Borrowing Capacity from the 10K sentences.
Your response should only include the answer. Do not provide any further explanation.
Here are some examples, complete the last one:
10K Sentence:
The credit agreement also provides that up to $500 million in commitments may be used for letters of credit.
Line Of Credit Facility Maximum Borrowing Capacity:
$500M

10K Sentence:
In March 2020, we upsized the Credit Agreement by $100 million, which matures July 2023, to $2.525 billion.
Line Of Credit Facility Maximum Borrowing Capacity:
$2.525B

10K Sentence:
We prepared our impairment test as of October 1, 2022 and determined that the fair values of each of our reporting units exceeded net book value by more than 50%. Among our reporting units, the narrowest difference between the calculated fair value and net book value was in our Principal Markets segment's Canada reporting unit, whose calculated fair value exceeded its net book value by 53%. Future developments related to macroeconomic factors, including increases to the discount rate used, or changes to other inputs and assumptions, including revenue growth, could reduce the fair value of this and/or other reporting units and lead to impairment. There were no goodwill impairment losses recorded for the nine months ended December 31, 2022. Cumulatively, the Company has recorded $469 million in goodwill impairment charges within its former EMEA ($293 million) and current United States ($176 million) reporting units. Revolving Credit Agreement In October 2021, we entered into a $3.15 billion multi-currency revolving credit agreement (the "Revolving Credit Agreement") for our future liquidity needs. The Revolving Credit Agreement expires, unless extended, in October 2026. Interest rates on borrowings under the Revolving Credit Agreement are based on prevailing market interest rates, plus a margin, as further described in the Revolving Credit Agreement. The total expense recorded by the Company for the Revolving Credit Agreement was not material in any of the periods presented. We may voluntarily prepay borrowings under the Revolving Credit Agreement without premium or penalty, subject to customary "breakage" costs. The Revolving Credit Agreement includes certain customary mandatory prepayment provisions. Interest on Debt Interest expense for the three and nine months ended December 31, 2022 was $27 million and $65 million, compared to $18 million and $50 million for the three and nine months ended December 31, 2021. Most of the interest for the pre-Separation period presented in the historical Consolidated Income Statement reflects the allocation of interest expense associated with debt issued by IBM from which a portion of the proceeds benefited Kyndryl.
Line Of Credit Facility Maximum Borrowing Capacity:
&lt;|assistant|&gt;

</code></pre>
                <p>The following screenshot shows the output that is generated by the granite-13b-chat-v2 foundation model when this prompt is submitted by using the Prompt Lab. The output is <code>$3.15B</code>.</p>
                <p><img src="images/fm-granite-chat-extract.png" alt="Shows granite-13b-chat-v2 in Freeform mode of the Prompt Lab classifying user feedback" style="max-width:90%;height:auto;width:auto"></p>
              </li>
            </ul>
          </section>
        </section>
        <section id="section-learn-more">
          <h2 id="learn-more">Learn more</h2>
          <p>To learn more about the granite-13b-chat-v2 foundation model, read the following resources:</p>
          <ul>
            <li>
              <p><a href="https://www.ibm.com/blog/watsonx-tailored-generative-ai/" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">Model information</a></p>
            </li>
            <li>
              <p><a href="https://www.ibm.com/downloads/cas/X9W4O6BM" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">Research paper</a></p>
            </li>
            <li>
              <p><a href="https://dataplatform.cloud.ibm.com/wx/samples/models/ibm/granite-13b-chat-v2?context=wx" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">Model card</a></p>
            </li>
          </ul>
          <p><strong>Parent topic:</strong> <a href="fm-models-ibm.html">IBM foundation models</a></p>
        </section>
      </section>
    </article>
  </main>

<script type="text/javascript"  src="/DEWsG3/JOI980/yQyM/iRupUf/mVarA/YDJa4kXtLhb7/LwkITClvAg/Wgkg/WmBVIDs"></script><link rel="stylesheet" type="text/css"  href="/_sec/cp_challenge/sec-4-4.css">
                                        <script  src="/_sec/cp_challenge/sec-cpt-4-4.js" async defer></script>
                                        <div id="sec-overlay" style="display:none;">
                                        <div id="sec-container">
                                        </div>
                                      </div></body></html>