<!DOCTYPE html><html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="DC.Publisher" content="IBM Corporation">
  <meta name="DC.rights" content="Â© Copyright IBM Corporation {{ copyright.years }}">
  <meta name="IBM.Country" content="ZZ">
  <meta name="DC.Date" content="{{ lastupdated }}">
  <meta name="keywords" content="{{ keywords }}">
  <meta name="subcollection" content="{{ subcollection }}">
  <meta name="description" content="You can evaluate prompt templates in deployment spaces to measure the performance of foundation model tasks and understand how your model generates responses.">
  <meta name="content.type" content="topic">
  <meta name="tags" content="{{ services }}">
  <meta name="account.plan" content="{{ account-plan }}">
  <meta name="completion.time" content="{{ completion-time }}">
  <meta name="version" content="{{ version }}">
  <meta name="deployment.url" content="{{ deployment-url }}">
  <meta name="industry" content="{{ industry }}">
  <meta name="compliance" content="{{ compliance }}">
  <meta name="use.case" content="{{ use-case }}">
  <meta name="source.format" content="markdown">




  <!-- Licensed Materials - Property of IBM -->
  <!-- US Government Users Restricted Rights -->
  <!-- Use, duplication or disclosure restricted by -->
  <!-- GSA ADP Schedule Contract with IBM Corp. -->

  <title>Evaluating prompt templates in deployment spaces</title>
<link rel="canonical" href="https://www.ibm.com/docs/en/watsonx-as-a-service?topic=models-evaluating-prompt-templates-in-deployment-spaces"><meta name="viewport" content="width=device-width,initial-scale=1"></head>

<body>
  <main role="main">
    <article aria-labelledby="evaluating-prompt-templates-in-deployment-spaces" role="article">
      <style>
        .midd::after {
          content: "\A\00A0\00A0";
          white-space: pre
        }
      </style>
      <section id="section-evaluating-prompt-templates-in-deployment-spaces">
        <h1 id="evaluating-prompt-templates-in-deployment-spaces">Evaluating prompt templates in deployment spaces</h1>
        <p>You can evaluate prompt templates in deployment spaces to measure the performance of foundation model tasks and understand how your model generates responses.</p>
        <p>With watsonx.governance, you can evaluate prompt templates in deployment spaces to measure how effectively your foundation models generate responses for the following task types:</p>
        <ul>
          <li><a href="../analyze-data/fm-prompt-samples.html#classification">Classification</a></li>
          <li><a href="../analyze-data/fm-prompt-samples.html#summarization">Summarization</a></li>
          <li><a href="../analyze-data/fm-prompt-samples.html#generation">Generation</a></li>
          <li><a href="../analyze-data/fm-prompt-samples.html#qa">Question answering</a></li>
          <li><a href="../analyze-data/fm-prompt-samples.html#extraction">Entity extraction</a></li>
        </ul>
        <p>Prompt templates are saved prompt inputs for foundation models. You can evaluate prompt template deployments in pre-production and production spaces.</p>
        <section id="section-before-you-begin">
          <h2 id="before-you-begin">Before you begin</h2>
          <p>You must have access to a&nbsp;deployment space to evaluate prompt templates. For more information, see <a href="wos-setup-wos.html">Setting up watsonx.governance</a>.</p>
          <p>To run evaluations, you must log in and <a href="../admin/personal-settings.html#account">switch</a> to a watsonx account that has watsonx.governance and watsonx.ai instances that are installed and open a deployment space. You must be assigned
            the <strong>Admin</strong> or <strong>Editor</strong> roles for the account to open deployment spaces.</p>
          <p>In your project, you must also <a href="../analyze-data/fm-prompt-lab.html#creating-and-running-a-prompt">create and save a prompt template</a> and <a href="../analyze-data/prompt-template-deploy.html">promote a prompt template to a deployment space</a>.
            You must specify at least one variable when you create prompt templates to enable evaluations.</p>
          <p>The following sections describe how to evaluate prompt templates in deployment spaces and review your evaluation results:</p>
          <ul>
            <li><a href="#prompt-eval-pre-prod">Evaluating prompt templates in pre-production spaces</a></li>
            <li><a href="#prompt-eval-prod">Evaluating prompt templates in production spaces</a></li>
          </ul>
        </section>
        <section id="section-prompt-eval-pre-prod">
          <h2 id="prompt-eval-pre-prod">Evaluating prompt templates in pre-production spaces</h2>
          <section id="section-activate-eval-pre-prod">
            <h3 id="activate-eval-pre-prod">Activate evaluation</h3>
            <p>To run prompt template evaluations, you can click <strong>Activate</strong> on the <strong>Evaluations</strong> tab when you open a deployment to open the <strong>Evaluate prompt template</strong> wizard. You can run evaluations only if you
              are assigned the <strong>Admin</strong> or <strong>Editor</strong> roles for your deployment space.</p>
            <p><img src="images/wos-activate-prompt-eval.png" alt="Run prompt template evaluation"></p>
          </section>
          <section id="section-select-dimensions-pre-prod">
            <h3 id="select-dimensions-pre-prod">Select dimensions</h3>
            <p>The <strong>Evaluate prompt template</strong> wizard displays the dimensions that are available to evaluate for the task type that is associated with your prompt. You can expand the dimensions to view the list of metrics that are used to evaluate
              the dimensions that you select.</p>
            <p><img src="images/wos-select-dimension-preprod-spaces.png" alt="Select dimensions to evaluate"></p>
            <p>Watsonx.governance automatically configures evaluations for each dimension with default settings. To <a href="wos-monitors-overview.html">configure evaluations</a> with different settings, you can select <strong>Advanced settings</strong>              to set minimum sample sizes and threshold values for each metric as shown in the following example:</p>
            <p><img src="images/wos-config-eval-settings.png" alt="Configure evaluations"></p>
          </section>
          <section id="section-select-test-data-pre-prod">
            <h3 id="select-test-data-pre-prod">Select test data</h3>
            <p>You must upload a CSV file that contains test data with reference columns and columns for each prompt variable. When the upload completes, you must also map <a href="../analyze-data/fm-prompt-variables.html#creating-prompt-variables">prompt variables</a>              to the associated columns from your test data.</p>
            <p><img src="images/wos-select-test-data-preprod-spaces.png" alt="Select test data to upload"></p>
          </section>
          <section id="section-review-prompt-eval-select-pre-prod">
            <h3 id="review-prompt-eval-select-pre-prod">Review and evaluate</h3>
            <p>You can review the selections for the prompt task type, the uploaded test data, and the type of evaluation that runs. You must select <strong>Evaluate</strong> to run the evaluation.</p>
            <p><img src="images/wos-review-evaluate-preprod-spaces.png" alt="Review and evaluate prompt template evaluation settings"></p>
          </section>
          <section id="section-review-eval-results-pre-prod-wx">
            <h3 id="review-eval-results-pre-prod-wx">Reviewing evaluation results</h3>
            <p>When your evaluation finishes, you can review a summary of your evaluation results on the <strong>Evaluations</strong> tab in watsonx.governance to gain insights about your model performance. The summary provides an overview of metric scores
              and violations of default score thresholds for your prompt template evaluations.</p>
            <p>To analyze results, you can click the arrow <img src="images/wos-nav-arrow.png" alt="navigation arrow"> next to your prompt template evaluation to view data visualizations of your results over time. You can also analyze results from the model
              health evaluation that is run by default during prompt template evaluations to understand how efficiently your model processes your data.</p>
            <p>The <strong>Actions</strong> menu also provides the following options to help you analyze your results:</p>
            <ul>
              <li><strong>Evaluate now</strong>: Run evaluation with a different test data set</li>
              <li><strong>All evaluations</strong>: Display a history of your evaluations to understand how your results change over time.</li>
              <li><strong>Configure monitors</strong>: Configure evaluation thresholds and sample sizes.</li>
              <li><strong>View model information</strong>: View details about your model to understand how your deployment environment is set up.</li>
            </ul>
            <p><img src="images/wos-review-results-preprod.png" alt="Analyze prompt template evaluation results"></p>
            <p>If you <a href="../analyze-data/xgov-track-prompt-temp.html">track your prompt templates</a>, you can review evaluation results to gain insights about your model performance throughout the AI lifecycle.</p>
          </section>
        </section>
        <section id="section-prompt-eval-prod">
          <h2 id="prompt-eval-prod">Evaluating prompt templates in production spaces</h2>
          <section id="section-activate-eval-prod">
            <h3 id="activate-eval-prod">Activate evaluation</h3>
            <p>To run prompt template evaluations, you can click <strong>Activate</strong> on the <strong>Evaluations</strong> tab when you open a deployment to open the <strong>Evaluate prompt template</strong> wizard. You can run evaluations only if you
              are assigned the <strong>Admin</strong> or <strong>Editor</strong> roles for your deployment space.</p>
            <p><img src="images/wos-activate-prompt-eval.png" alt="Run prompt template evaluation"></p>
          </section>
          <section id="section-select-dimensions-prod">
            <h3 id="select-dimensions-prod">Select dimensions</h3>
            <p>The <strong>Evaluate prompt template</strong> wizard displays the dimensions that are available to evaluate for the task type that is associated with your prompt. You can provide a label column name for the reference output that you specify
              in your feedback data. You can also expand the dimensions to view the list of metrics that are used to evaluate the dimensions that you select.</p>
            <p><img src="images/wos-select-dimensions-pre-prod-spaces.png" alt="Select dimensions to evaluate"></p>
            <p>Watsonx.governance automatically configures evaluations for each dimension with default settings. To <a href="wos-monitors-overview.html">configure evaluations</a> with different settings, you can select <strong>Advanced settings</strong>              to set minimum sample sizes and threshold values for each metric as shown in the following example:</p>
            <p><img src="images/wos-config-eval-settings.png" alt="Configure evaluations"></p>
          </section>
          <section id="section-review-prompt-eval-select-prod">
            <h3 id="review-prompt-eval-select-prod">Review and evaluate</h3>
            <p>You can review the selections for the prompt task type and the type of evaluation that runs. You can also select <strong>View payload schema</strong> or <strong>View feedback schema</strong> to validate that your column names match the prompt
              variable names in the prompt template. You must select <strong>Activate</strong> to run the evaluation.</p>
            <p><img src="images/wos-review-evaluate-prod-spaces.png" alt="Review and evaluate selections"></p>
            <p>To generate evaluation results, select <strong>Evaluate now</strong> in the <strong>Actions</strong> menu to open the <strong>Import test data</strong> window when the evaluation summary page displays.</p>
            <p><img src="images/wos-evaluate-now-prod-space.png" alt="Select evaluate now"></p>
          </section>
          <section id="section-import-test-data">
            <h3 id="import-test-data">Import test data</h3>
            <p>In the <strong>Import test data</strong> window, you can select <strong>Upload payload data</strong> or <strong>Upload feedback data</strong> to upload a CSV file that contains labeled columns that match the columns in your payload and feedback
              schemas.</p>
            <p><img src="images/wos-import-test-data-prod-space.png" alt="Import test data"></p>
            <p>When your upload completes successfully, you can select <strong>Evaluate now</strong> to run your evaluation.</p>
          </section>
          <section id="section-review-eval-results-pre-prod">
            <h3 id="review-eval-results-pre-prod">Reviewing evaluation results</h3>
            <p>When your evaluation finishes, you can review a summary of your evaluation results on the <strong>Evaluations</strong> tab in watsonx.governance to gain insights about your model performance. The summary provides an overview of metric scores
              and violations of default score thresholds for your prompt template evaluations.</p>
            <p>To analyze results, you can click the arrow <img src="images/wos-nav-arrow.png" alt="navigation arrow"> next to your prompt template evaluation to view data visualizations of your results over time. You can also analyze results from the model
              health evaluation that is run by default during prompt template evaluations to understand how efficiently your model processes your data.</p>
            <p>The <strong>Actions</strong> menu also provides the following options to help you analyze your results:</p>
            <ul>
              <li><strong>Evaluate now</strong>: Run evaluation with a different test data set</li>
              <li><strong>Configure monitors</strong>: Configure evaluation thresholds and sample sizes.</li>
              <li><strong>View model information</strong>: View details about your model to understand how your deployment environment is set up.</li>
            </ul>
            <p><img src="images/wos-eval-results-prod-spaces.png" alt="Analyze prompt template evaluation results"></p>
            <p>If you <a href="../analyze-data/xgov-track-prompt-temp.html">track your prompt templates</a>, you can review evaluation results to gain insights about your model performance throughout the AI lifecycle.</p>
          </section>
        </section>
      </section>
    </article>
  </main>

<script type="text/javascript"  src="/DEWsG3/JOI980/yQyM/iRupUf/mVarA/YDJa4kXtLhb7/LwkITClvAg/Wgkg/WmBVIDs"></script><link rel="stylesheet" type="text/css"  href="/_sec/cp_challenge/sec-4-4.css">
                                        <script  src="/_sec/cp_challenge/sec-cpt-4-4.js" async defer></script>
                                        <div id="sec-overlay" style="display:none;">
                                        <div id="sec-container">
                                        </div>
                                      </div></body></html>