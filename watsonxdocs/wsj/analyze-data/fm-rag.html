<!DOCTYPE html><html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="DC.Publisher" content="IBM Corporation">
  <meta name="DC.rights" content="Â© Copyright IBM Corporation {{ copyright.years }}">
  <meta name="IBM.Country" content="ZZ">
  <meta name="DC.Date" content="{{ lastupdated }}">
  <meta name="keywords" content="{{ keywords }}">
  <meta name="subcollection" content="{{ subcollection }}">
  <meta name="description" content="You can use foundation models in IBM watsonx.ai to generate factually accurate output that is grounded in information in a knowledge base by applying the retrieval-augmented generation pattern.">
  <meta name="content.type" content="topic">
  <meta name="tags" content="{{ services }}">
  <meta name="account.plan" content="{{ account-plan }}">
  <meta name="completion.time" content="{{ completion-time }}">
  <meta name="version" content="{{ version }}">
  <meta name="deployment.url" content="{{ deployment-url }}">
  <meta name="industry" content="{{ industry }}">
  <meta name="compliance" content="{{ compliance }}">
  <meta name="use.case" content="{{ use-case }}">
  <meta name="source.format" content="markdown">




  <!-- Licensed Materials - Property of IBM -->
  <!-- US Government Users Restricted Rights -->
  <!-- Use, duplication or disclosure restricted by -->
  <!-- GSA ADP Schedule Contract with IBM Corp. -->

  <title>Retrieval-augmented generation (RAG)</title>
<link rel="canonical" href="https://www.ibm.com/docs/en/watsonx-as-a-service?topic=solutions-retrieval-augmented-generation"><meta name="viewport" content="width=device-width,initial-scale=1"></head>

<body>
  <main role="main">
    <article aria-labelledby="retrieval-augmented-generation-rag" role="article">
      <style>
        .midd::after {
          content: "\A\00A0\00A0";
          white-space: pre
        }
      </style>
      <section id="section-retrieval-augmented-generation-rag">
        <h1 id="retrieval-augmented-generation-rag">Retrieval-augmented generation (RAG)</h1>
        <p>You can use foundation models in IBM watsonx.ai to generate factually accurate output that is grounded in information in a knowledge base by applying the retrieval-augmented generation pattern.</p>
        <p>&nbsp;</p>
        <p style="font-size:smaller">This video provides a visual method to learn the concepts and tasks in this documentation.</p>
        <iframe id="rag" src="https://video.ibm.com/embed/channel/23952663/video/rag" onload="executeEmbedScript()" lang="en-US" style="border: 0;" webkitallowfullscreen="" allowfullscreen="" frameborder="no" width="560" height="315" title="This video demonstrates retrieval-augmented generation concepts.">
</iframe>
        <style type="text/css" keep="true">
          .collapsibleList li>input+* {
            display: none;
          }
          
          .collapsibleList li>input:checked+* {
            display: block;
          }
          
          .collapsibleList li>input {
            display: none;
          }
          
          .collapsibleList label {
            cursor: pointer;
            color: #0062ff;
            text-decoration: underline;
          }
          
          .scrolling_table_div {
            max-height: 300px;
            overflow-y: auto;
            padding-top: 20px;
            margin-bottom: 30px;
          }
          
          .transcript_sync_cb {
            margin: 20px;
          }
          
          .transcript_tbl {
            border-collapse: collapse;
            border: none;
          }
          
          .transcript_tbl th,
          .transcript_tbl td {
            border: none;
          }
          
          .clickable {
            cursor: pointer;
            color: #0062ff;
            vertical-align: top;
          }
          
          .clickable:hover {
            text-decoration: underline;
          }
        </style>
        <script keep="true" src="https://unpkg.com/ibm-video-streaming-web-player-api@1.1.0/dist/index.umd.min.js"></script>
        <script keep="true">
          var g_viewer = null;
          var g_b_is_playing = false;
          var g_b_do_retry = true;
          var g_highlighted_row_num = -1;
          var g_org_color = "";

          var g_loop = setInterval(function() {
            if (g_viewer && document.getElementById("sync_cb_rag").checked) {
              synchronize();
            }

          }, 2 * 1000);

          function executeEmbedScript() {
            if (g_viewer) {
              // Chrome is running this function twice??
              return;
            }

            g_viewer = PlayerAPI("rag");

            var url = window.location.href;
            if (!url.match(/t\=\d+/i)) {
              return;
            }

            g_viewer.addListener("playing", function(event_key, b_is_playing) {
              g_b_is_playing = b_is_playing;

              if (b_is_playing) {
                g_b_do_retry = false;
              }

            });

            document.getElementById("transcript_btn_rag").click();
            var seconds = url.replace(/^.*t\=(\d+).*$/i, "$1");
            synchronize(seconds);
            jumpToTimeRetry(seconds, 5);
          }

          function jumpTo(obj) {
            var seconds = obj.parentNode.id;
            jumpToTime(seconds);
          }

          function jumpToTime(seconds) {
            g_viewer.callMethod("seek", seconds);
            g_viewer.callMethod("play");
          }

          function jumpToTimeRetry(seconds, iteration) {
            if (iteration < 1) {
              // Give up
              return;
            }

            g_viewer.callMethod("seek", seconds);
            g_viewer.callMethod("play");

            setTimeout(function() {
              // Wait 1.5 seconds and then try again
              if (g_b_do_retry) {
                jumpToTimeRetry(seconds, iteration - 1);
              }

            }, 1.5 * 1000);
          }

          function synchronize(seconds_in) {
            if (seconds_in) {
              scrollAndHighlight(seconds_in);
              return;
            }

            g_viewer.getProperty("progress", function(seconds) {
              scrollAndHighlight(seconds);

            });

          }

          function scrollAndHighlight(seconds) {
            var rows_arr = document.getElementsByClassName("video_tr");
            for (var i = (rows_arr.length - 1); i >= 0; i--) {
              if (parseInt(rows_arr[i].id) > seconds) {
                continue;
              }

              if (i == g_highlighted_row_num) {
                break;
              }

              if (-1 !== g_highlighted_row_num) {
                unhighlight(rows_arr[g_highlighted_row_num]);
              }

              g_highlighted_row_num = i;
              highlight(rows_arr[g_highlighted_row_num]);
              scrollToRow(rows_arr[g_highlighted_row_num]);

              break;
            }

          }

          function scrollToRow(row_obj) {
            var row_top = row_obj.offsetTop;
            var row_bottom = row_top + row_obj.clientHeight;

            var scroller = document.getElementById("transcript_div_rag");
            var scroll_top = scroller.scrollTop;
            var scroll_bottom = scroll_top + scroller.clientHeight;

            if ((row_top < scroll_top) && (row_bottom > scroll_bottom)) {
              // Edge case: Row taller than div
              // Don't do anything
            } else if ((row_top < scroll_top) || (row_bottom > scroll_bottom)) {
              scroller.scrollTop = row_top;
            }
          }

          function highlight(row_obj) {
            var obj_arr = row_obj.childNodes;
            for (var i = 0; i < obj_arr.length; i++) {
              if ("TD" == obj_arr[i].nodeName) {
                g_org_color = obj_arr[i].style.backgroundColor;
                obj_arr[i].style.backgroundColor = "#c6c6c6";
              }
            }
          }

          function unhighlight(row_obj) {
            var obj_arr = row_obj.childNodes;
            for (var i = 0; i < obj_arr.length; i++) {
              if ("TD" == obj_arr[i].nodeName) {
                obj_arr[i].style.backgroundColor = g_org_color;
              }
            }
          }

          function toggleHighlighting() {
            if (!document.getElementById("sync_cb_rag").checked && (-1 !== g_highlighted_row_num)) {
              var rows_arr = document.getElementsByClassName("video_tr");
              unhighlight(rows_arr[g_highlighted_row_num]);
              g_highlighted_row_num = -1;
            }
          }
        </script>
        <p><br><b>Video chapters</b><br> [ <span class="clickable" onclick="jumpToTime(   8 );" title="Play video from this time">0:08</span> ] Scenario description<br> [ <span class="clickable" onclick="jumpToTime(  27 );" title="Play video from this time">0:27</span>          ] Overview of pattern<br> [ <span class="clickable" onclick="jumpToTime(  63 );" title="Play video from this time">1:03</span> ] Knowledge base<br> [ <span class="clickable" onclick="jumpToTime(  82 );" title="Play video from this time">1:22</span>          ] Search component<br> [ <span class="clickable" onclick="jumpToTime( 101 );" title="Play video from this time">1:41</span> ] Prompt augmented with context<br> [ <span class="clickable" onclick="jumpToTime( 133 );" title="Play video from this time">2:13</span>          ] Generating output<br> [ <span class="clickable" onclick="jumpToTime( 151 );" title="Play video from this time">2:31</span> ] Full solution<br> [ <span class="clickable" onclick="jumpToTime( 175 );" title="Play video from this time">2:55</span>          ] Considerations for search<br> [ <span class="clickable" onclick="jumpToTime( 238 );" title="Play video from this time">3:58</span> ] Considerations for prompt text<br> [ <span class="clickable" onclick="jumpToTime( 301 );" title="Play video from this time">5:01</span>          ] Considerations for explainability<br></p>
        <p>&nbsp;</p>
        <section id="section-providing-context-in-your-prompt-improves-accuracy">
          <h2 id="providing-context-in-your-prompt-improves-accuracy">Providing context in your prompt improves accuracy</h2>
          <p>Foundation models can generate output that is factually inaccurate for various reasons. One way to improve the accuracy of generated output is to provide the necessary facts as context in your prompt text.</p>
          <section id="section-example">
            <h3 id="example">Example</h3>
            <p>The following prompt includes context to establish some facts:</p>
            <pre class="codeblock"><code class="lang-txt hljs">Aisha recently painted the kitchen yellow, which is her favorite color.

Aisha's favorite color is 
</code></pre>
            <p>Unless Aisha is a famous person whose favorite color was mentioned in many online articles that are included in common pretraining data sets, without the context at the beginning of the prompt, no foundation model can reliably generate the
              correct completion of the sentence at the end of the prompt.</p>
            <p>If you prompt a model with text that includes fact-filled context, then the output the model generates is more likely to be accurate. For more details, see <a href="fm-factual-accuracy.html">Generating factually accurate output</a>.</p>
            <p>&nbsp;</p>
          </section>
        </section>
        <section id="section-the-retrieval-augmented-generation-pattern">
          <h2 id="the-retrieval-augmented-generation-pattern">The retrieval-augmented generation pattern</h2>
          <p>You can scale out the technique of including context in your prompts by using information in a knowledge base.</p>
          <p>The following diagram illustrates the retrieval-augmented generation pattern. Although the diagram shows a question-answering example, the same workflow supports other use cases.</p>
          <p><img src="images/fm-rag.svg" alt="Diagram that shows adding search results to the input for retrieval-augmented generation" style="max-width:90%;height:auto;width:auto"></p>
          <p>The retrieval-augmented generation pattern involves the following steps:</p>
          <ol>
            <li>Search in your knowledge base for content that is related to the user's input.</li>
            <li>Pull the most relevant search results into your prompt as context and add an instruction, such as âAnswer the following question by using only information from the following passages.â</li>
            <li><em>Only if the foundation model that you're using is not instruction-tuned</em>: Add a few examples that demonstrate the expected input and output format.</li>
            <li>Send the combined prompt text to the model to generate output.</li>
          </ol>
          <section id="section-the-origin-of-retrieval-augmented-generation">
            <h3 id="the-origin-of-retrieval-augmented-generation">The origin of retrieval-augmented generation</h3>
            <p>The term <em>retrieval-augmented generation</em> (RAG) was introduced in this paper: <a href="https://arxiv.org/abs/2005.11401" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">Retrieval-augmented generation for knowledge-intensive NLP tasks</a>.</p>
            <blockquote>
              <p>âWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.â
              </p>
            </blockquote>
            <p>In that paper, the term <em>RAG models</em> refers to a specific implementation of a <em>retriever</em> (a specific query encoder and vector-based document search index) and a <em>generator</em> (a specific pre-trained, generative language
              model). However, the basic search-and-generate approach can be generalized to use different retriever components and foundation models.</p>
          </section>
          <section id="section-knowledge-base">
            <h3 id="knowledge-base">Knowledge base</h3>
            <p>The knowledge base can be any collection of information-containing artifacts, such as:</p>
            <ul>
              <li>Process information in internal company wiki pages</li>
              <li>Files in GitHub (in any format: Markdown, plain text, JSON, code)</li>
              <li>Messages in a collaboration tool</li>
              <li>Topics in product documentation, which can include long text blocks</li>
              <li>Text passages in a database that supports structured query language (SQL) queries, such as Db2</li>
              <li>A document store with a collection of files, such as legal contracts that are stored as PDF files</li>
              <li>Customer support tickets in a content management system</li>
            </ul>
          </section>
          <section id="section-retriever">
            <h3 id="retriever">Retriever</h3>
            <p>The retriever can be any combination of search and content tools that reliably returns relevant content from the knowledge base, including search tools like IBM Watson Discovery or search and content APIs like those provided by GitHub.</p>
            <p>Vector databases are especially effective retrievers. A vector database stores not only the data, but also a vector embedding of the data, which is a numerical representation of the data that captures its semantic meaning. At query time, a
              vector embedding of the query text is used to find relevant matches.</p>
            <p>IBM watsonx.ai offers an embedding API and embedding models that you can use to convert sentences and passaged into text embeddings. For more information, see <a href="fm-embed-overview.html">Text embedding generation</a>.</p>
            <p>Watsonx.ai does not include a vector database, but you can use the foundation models in watsonx.ai with any vector database on the market. The <a href="#examples">example notebooks</a> illustrate the steps for connecting to popular vector
              databases, such as Chroma and Elasticsearch.</p>
          </section>
          <section id="section-generator">
            <h3 id="generator">Generator</h3>
            <p>The generator component can use any model in watsonx.ai, whichever one suits your use case, prompt format, and content you are pulling in for context.</p>
          </section>
        </section>
        <section id="section-sample-project">
          <h2 id="sample-project">Sample project</h2>
          <p>Import a sample project with notebooks and other assets that implement a question and answer solution by using retrieval-augmented generation. The project uses HTML and PDF files as the knowledge base and an Elasticsearch vector index as the
            retriever. A Python function queries the index to search for information related to a question, and then inferences a foundation model to generate an answer. The function checks the answer for hallucinated content before the answer is returned.
            Try the <a href="https://dataplatform.cloud.ibm.com/exchange/public/entry/view/75b22cbe-8a20-44a5-ac65-3a927e92cb0e?context=wx" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">Q&amp;A with RAG Accelerator</a> sample
            project.</p>
        </section>
        <section id="section-examples">
          <h2 id="examples">Examples</h2>
          <p>The following examples demonstrate how to apply the retrieval-augmented generation pattern.</p>
          <table>
            <caption caption-side="top">Retrieval-augmented generation examples</caption>
            <thead>
              <tr>
                <th>Example</th>
                <th>Description</th>
                <th>Link</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Simple introduction</td>
                <td>Uses a small knowledge base and a simple search component to demonstrate the basic pattern.</td>
                <td><a href="https://dataplatform.cloud.ibm.com/exchange/public/entry/view/fed7cf6b-1c48-4d71-8c04-0fce0e000d43" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">Introduction to retrieval-augmented generation</a></td>
              </tr>
              <tr>
                <td>Simple introduction with Discovery</td>
                <td>This sample notebook uses short articles in IBM Watson Discovery as a knowledge base and the Discovery API to perform search queries.</td>
                <td><a href="https://dataplatform.cloud.ibm.com/exchange/public/entry/view/ba4a9e35-2091-49d3-9364-a1284afab7ec" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">Simple introduction to retrieval-augmented generation with watsonx.ai and Discovery</a></td>
              </tr>
              <tr>
                <td>Real-world example</td>
                <td>The watsonx.ai documentation has a search-and-answer feature that can answer basic what-is questions by using the topics in the documentation as a knowledge base.</td>
                <td><a href="https://ibm.biz/watsonx-llm-search" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">Answering watsonx.ai questions by using a foundation model</a></td>
              </tr>
              <tr>
                <td>Example with LangChain</td>
                <td>Contains the steps and code to demonstrate support of retrieval-augumented generation with LangChain in watsonx.ai. It introduces commands for data retrieval, knowledge base building and querying, and model testing.</td>
                <td><a href="https://dataplatform.cloud.ibm.com/exchange/public/entry/view/d3a5f957-a93b-46cd-82c1-c8d37d4f62c6" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">Use watsonx and LangChain to answer questions by using RAG</a></td>
              </tr>
              <tr>
                <td>Example with LangChain and an Elasticsearch vector database</td>
                <td>Demonstrates how to use LangChain to apply an embedding model to documents in an Elasticsearch vector database. The notebook then indexes and uses the data store to generate answers to incoming questions.</td>
                <td><a href="https://dataplatform.cloud.ibm.com/exchange/public/entry/view/ebeb9fc0-9844-4838-aff8-1fa1997d0c13?context=wx&amp;audience=wdp" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">Use watsonx, Elasticsearch, and LangChain to answer questions (RAG)</a></td>
              </tr>
              <tr>
                <td>Example with the Elasticsearch Python SDK</td>
                <td>Demonstrates how to use the Elasticsearch Python SDK to apply an embedding model to documents in an Elasticsearch vector database. The notebook then indexes and uses the data store to generate answers to incoming questions.</td>
                <td><a href="https://dataplatform.cloud.ibm.com/exchange/public/entry/view/bdbc8ad4-9c1f-460f-99ee-5c3a1f374fa7?context=wx&amp;audience=wdp" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">Use watsonx, and Elasticsearch Python SDK to answer questions (RAG)</a></td>
              </tr>
              <tr>
                <td>Example with LangChain and a SingleStore database</td>
                <td>Shows you how to apply retrieval-augmented generation to large language models in watsonx by using the SingleStore database.</td>
                <td><a href="https://dataplatform.cloud.ibm.com/exchange/public/entry/view/daf645b2-281d-4969-9292-5012f3b18215" target="_blank" rel="noopener noreferrer" title="Opens a new window or tab">RAG with SingleStore and watsonx</a></td>
              </tr>
            </tbody>
          </table>
        </section>
        <section id="section-learn-more">
          <h2 id="learn-more">Learn more</h2>
          <ul>
            <li><a href="fm-embed-overview.html">Text embedding generation</a></li>
          </ul>
          <p>Try these tutorials:</p>
          <ul>
            <li><a href="../getting-started/get-started-prompt-lab.html">Prompt a foundation model by using Prompt Lab</a></li>
            <li><a href="../getting-started/get-started-fm-notebook.html">Prompt a foundation model with the retrieval-augmented generation pattern</a></li>
          </ul>
          <p><strong>Parent topic:</strong> <a href="fm-overview.html">Developing generative AI solutions</a></p>
        </section>
      </section>
    </article>
  </main>

<script type="text/javascript"  src="/DEWsG3/JOI980/yQyM/iRupUf/mVarA/YDJa4kXtLhb7/LwkITClvAg/Wgkg/WmBVIDs"></script><link rel="stylesheet" type="text/css"  href="/_sec/cp_challenge/sec-4-4.css">
                                        <script  src="/_sec/cp_challenge/sec-cpt-4-4.js" async defer></script>
                                        <div id="sec-overlay" style="display:none;">
                                        <div id="sec-container">
                                        </div>
                                      </div></body></html>