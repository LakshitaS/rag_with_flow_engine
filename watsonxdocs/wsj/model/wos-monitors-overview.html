<!DOCTYPE html><html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="DC.Publisher" content="IBM Corporation">
  <meta name="DC.rights" content="Â© Copyright IBM Corporation {{ copyright.years }}">
  <meta name="IBM.Country" content="ZZ">
  <meta name="DC.Date" content="{{ lastupdated }}">
  <meta name="keywords" content="{{ keywords }}">
  <meta name="subcollection" content="{{ subcollection }}">
  <meta name="description" content="Configure watsonx.governance evaluations to generate insights about your model performance.">
  <meta name="content.type" content="topic">
  <meta name="tags" content="{{ services }}">
  <meta name="account.plan" content="{{ account-plan }}">
  <meta name="completion.time" content="{{ completion-time }}">
  <meta name="version" content="{{ version }}">
  <meta name="deployment.url" content="{{ deployment-url }}">
  <meta name="industry" content="{{ industry }}">
  <meta name="compliance" content="{{ compliance }}">
  <meta name="use.case" content="{{ use-case }}">
  <meta name="source.format" content="markdown">




  <!-- Licensed Materials - Property of IBM -->
  <!-- US Government Users Restricted Rights -->
  <!-- Use, duplication or disclosure restricted by -->
  <!-- GSA ADP Schedule Contract with IBM Corp. -->

  <title>Configuring model evaluations</title>
<link rel="canonical" href="https://www.ibm.com/docs/en/watsonx-as-a-service?topic=models-configuring-model-evaluations"><meta name="viewport" content="width=device-width,initial-scale=1"></head>

<body>
  <main role="main">
    <article aria-labelledby="configuring-model-evaluations" role="article">
      <style>
        .midd::after {
          content: "\A\00A0\00A0";
          white-space: pre
        }
      </style>
      <section id="section-configuring-model-evaluations">
        <h1 id="configuring-model-evaluations">Configuring model evaluations</h1>
        <p>Configure watsonx.governance evaluations to generate insights about your model performance.</p>
        <section id="section-configuring-model-evaluations-with-watson-openscale">
          <h2 id="configuring-model-evaluations-with-watson-openscale">Configuring model evaluations with Watson OpenScale</h2>
          <p>If you're using the Watson OpenScale service, you can configure the following types of evaluations:</p>
          <ul>
            <li><a href="wos-monitor-accuracy.html">Quality</a><br> Evaluates how well your model predicts correct outcomes that match labeled test data.</li>
            <li><a href="wos-monitor-fairness.html">Fairness</a><br> Evaluates whether your model produces biased outcomes that provide favorable results for one group over another.</li>
            <li><a href="wos-monitor-drift.html">Drift</a><br> Evaluates how your model changes in accuracy and data consistency by comparing recent transactions to your training data.</li>
            <li><a href="wos-driftv2-config.html">Drift v2</a><br> Evaluates changes in your model output, the accuracy of your predictions, and the distribution of your input data.</li>
            <li><a href="wos-model-health-metrics.html">Model health</a><br> Evaluates how efficiently your model deployment processes your transactions.</li>
          </ul>
          <p>You can also create <a href="wos-custom-metrics.html">custom evaluations and metrics</a> to generate a greater variety of insights about your model performance.</p>
          <p>Each evaluation generates metrics that you can analyze to gain insights about your model performance. For more information see, <a href="wos-insight-timechart.html">Reviewing evaluation results</a>.</p>
        </section>
        <section id="section-configuring-model-evaluations-with-watsonxgovernance">
          <h2 id="configuring-model-evaluations-with-watsonxgovernance">Configuring model evaluations with watsonx.governance</h2>
          <p>If you're using the watsonx.governance service, you can configure the following types of evaluations:</p>
          <ul>
            <li><a href="wos-monitor-accuracy.html">Quality</a><br> Evaluates how well your model predicts correct outcomes that match labeled test data.</li>
            <li><a href="wos-driftv2-config.html">Drift v2</a><br> Evaluates changes in your model output, the accuracy of your predictions, and the distribution of your input data</li>
            <li><a href="wos-monitor-gen-quality.html">Generative AI quality</a><br> Measures how well your foundation model performs tasks</li>
            <li><a href="wos-model-health-metrics.html">Model health</a><br> Evaluates how efficiently your model deployment processes your transactions.</li>
          </ul>
          <p><strong>Parent topic:</strong> <a href="getting-started.html">Evaluating AI models with Watson OpenScale</a></p>
        </section>
      </section>
    </article>
  </main>

<script type="text/javascript"  src="/DEWsG3/JOI980/yQyM/iRupUf/mVarA/YDJa4kXtLhb7/LwkITClvAg/Wgkg/WmBVIDs"></script><link rel="stylesheet" type="text/css"  href="/_sec/cp_challenge/sec-4-4.css">
                                        <script  src="/_sec/cp_challenge/sec-cpt-4-4.js" async defer></script>
                                        <div id="sec-overlay" style="display:none;">
                                        <div id="sec-container">
                                        </div>
                                      </div></body></html>